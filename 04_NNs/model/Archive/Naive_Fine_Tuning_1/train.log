2025-06-13 16:07:32,824 - ==== Skipping Regular LSTM Training Phase ====
2025-06-13 16:07:32,825 - ==== Incremental Training Phase ====
2025-06-13 16:08:19,814 - Base train IDs: ['03', '05', '07', '27']
2025-06-13 16:08:19,817 - Base train size: 92079
2025-06-13 16:08:19,838 - Base val IDs: ['01']
2025-06-13 16:08:19,838 - Base val size: 28612
2025-06-13 16:08:19,838 - Update1 train IDs: ['21', '23', '25']
2025-06-13 16:08:19,838 - Update1 train size: 65674
2025-06-13 16:08:19,838 - Update1 val IDs: ['19']
2025-06-13 16:08:19,838 - Update1 val size: 23120
2025-06-13 16:08:19,838 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-13 16:08:19,838 - Update2 train size: 47891
2025-06-13 16:08:19,838 - Update2 val IDs: ['13']
2025-06-13 16:08:19,857 - Update2 val size: 6445
2025-06-13 16:08:24,081 - Test cell ID: 17
2025-06-13 16:08:24,081 - Test size: 22872
2025-06-13 16:08:24,081 - Test base size: 11139
2025-06-13 16:08:24,081 - Test update1 size: 6312
2025-06-13 16:08:24,081 - Test update2 size: 5421
2025-06-13 16:08:24,081 - Scaling datasets with RobustScaler...
2025-06-13 16:08:24,162 - [Scaler Base] center: [ 3.31975183  0.         27.55116667], scale: [0.20016217 1.86108033 1.05333333]
2025-06-13 16:08:24,167 - Base train scaler fitted and datasets scaled.
2025-06-13 16:08:24,200 - [Scaler Update1] center: [ 3.3375475   0.09004367 27.77316667], scale: [0.189709   2.018227   1.66283333]
2025-06-13 16:08:24,200 - Update1 train scaler fitted and datasets scaled.
2025-06-13 16:08:24,245 - [Scaler Update2] center: [ 3.342422    0.17992967 27.75      ], scale: [0.18428363 1.77955483 1.81783333]
2025-06-13 16:08:24,246 - Update2 train scaler fitted and datasets scaled.
2025-06-13 16:08:24,708 - [task0] Training... (EWC=False, lambda=0.0000)
2025-06-13 16:09:41,489 - Epoch 1, Train Loss: 1.6381e-02, Val Loss: 1.0405e-03, LR: 1.0000e-04, Time: 75.31s
2025-06-13 16:11:11,048 - Epoch 2, Train Loss: 2.2660e-03, Val Loss: 5.1531e-04, LR: 1.0000e-04, Time: 89.51s
2025-06-13 16:12:44,666 - Epoch 3, Train Loss: 1.8063e-03, Val Loss: 4.2510e-04, LR: 1.0000e-04, Time: 93.58s
2025-06-13 16:14:36,185 - Epoch 4, Train Loss: 1.5928e-03, Val Loss: 4.1393e-04, LR: 1.0000e-04, Time: 111.48s
2025-06-13 16:16:13,389 - Epoch 5, Train Loss: 1.4164e-03, Val Loss: 4.0566e-04, LR: 1.0000e-04, Time: 97.15s
2025-06-13 16:17:58,977 - Epoch 6, Train Loss: 1.2523e-03, Val Loss: 4.6368e-04, LR: 1.0000e-04, Time: 105.57s
2025-06-13 16:19:39,370 - Epoch 7, Train Loss: 1.1080e-03, Val Loss: 4.0425e-04, LR: 1.0000e-04, Time: 100.38s
2025-06-13 16:22:01,576 - Epoch 8, Train Loss: 9.8832e-04, Val Loss: 4.3262e-04, LR: 1.0000e-04, Time: 142.18s
2025-06-13 16:24:07,435 - Epoch 9, Train Loss: 8.7714e-04, Val Loss: 4.1145e-04, LR: 1.0000e-04, Time: 125.83s
2025-06-13 16:25:57,893 - Epoch 10, Train Loss: 8.0787e-04, Val Loss: 3.9090e-04, LR: 1.0000e-04, Time: 110.43s
2025-06-13 16:27:47,466 - Epoch 11, Train Loss: 7.2745e-04, Val Loss: 4.0963e-04, LR: 1.0000e-04, Time: 109.52s
2025-06-13 16:29:32,440 - Epoch 12, Train Loss: 9.1573e-04, Val Loss: 5.0213e-04, LR: 1.0000e-04, Time: 104.96s
2025-06-13 16:31:13,052 - Epoch 13, Train Loss: 6.0403e-04, Val Loss: 4.9750e-04, LR: 1.0000e-04, Time: 100.60s
2025-06-13 16:32:54,090 - Epoch 14, Train Loss: 5.4870e-04, Val Loss: 4.5765e-04, LR: 1.0000e-04, Time: 101.03s
2025-06-13 16:34:41,487 - Epoch 15, Train Loss: 4.9014e-04, Val Loss: 3.8848e-04, LR: 1.0000e-04, Time: 107.38s
2025-06-13 16:36:22,352 - Epoch 16, Train Loss: 4.5220e-04, Val Loss: 4.7809e-04, LR: 1.0000e-04, Time: 100.83s
2025-06-13 16:38:04,889 - Epoch 17, Train Loss: 4.3250e-04, Val Loss: 5.4763e-04, LR: 1.0000e-04, Time: 102.52s
2025-06-13 16:39:47,941 - Epoch 18, Train Loss: 4.1337e-04, Val Loss: 5.3163e-04, LR: 1.0000e-04, Time: 103.04s
2025-06-13 16:41:30,466 - Epoch 19, Train Loss: 4.1321e-04, Val Loss: 3.4110e-04, LR: 1.0000e-04, Time: 102.51s
2025-06-13 16:43:32,932 - Epoch 20, Train Loss: 4.3819e-04, Val Loss: 4.0014e-04, LR: 1.0000e-04, Time: 122.44s
2025-06-13 16:46:52,599 - Epoch 21, Train Loss: 3.8756e-04, Val Loss: 5.9027e-04, LR: 1.0000e-04, Time: 199.64s
2025-06-13 16:49:46,286 - Epoch 22, Train Loss: 3.5188e-04, Val Loss: 4.0349e-04, LR: 1.0000e-04, Time: 173.64s
2025-06-13 16:52:22,117 - Epoch 23, Train Loss: 2.6849e-04, Val Loss: 4.0345e-04, LR: 1.0000e-04, Time: 155.79s
2025-06-13 16:54:15,494 - Epoch 24, Train Loss: 3.0957e-04, Val Loss: 4.5399e-04, LR: 1.0000e-04, Time: 113.36s
2025-06-13 16:56:15,871 - Epoch 25, Train Loss: 2.2406e-04, Val Loss: 6.5764e-04, LR: 5.0000e-05, Time: 120.35s
2025-06-13 16:58:23,549 - Epoch 26, Train Loss: 1.9431e-04, Val Loss: 5.9584e-04, LR: 5.0000e-05, Time: 127.66s
2025-06-13 17:00:24,433 - Epoch 27, Train Loss: 1.0873e-04, Val Loss: 1.1441e-03, LR: 5.0000e-05, Time: 120.86s
2025-06-13 17:02:44,638 - Epoch 28, Train Loss: 7.0931e-05, Val Loss: 6.7919e-04, LR: 5.0000e-05, Time: 140.18s
2025-06-13 17:05:11,206 - Epoch 29, Train Loss: 8.2285e-05, Val Loss: 1.8379e-03, LR: 5.0000e-05, Time: 146.53s
2025-06-13 17:07:20,513 - Epoch 30, Train Loss: 5.8922e-05, Val Loss: 2.5711e-03, LR: 5.0000e-05, Time: 129.26s
2025-06-13 17:09:17,934 - Epoch 31, Train Loss: 4.0239e-05, Val Loss: 2.2105e-03, LR: 2.5000e-05, Time: 117.40s
2025-06-13 17:11:32,258 - Epoch 32, Train Loss: 2.9168e-05, Val Loss: 2.2658e-03, LR: 2.5000e-05, Time: 134.29s
2025-06-13 17:13:34,349 - Epoch 33, Train Loss: 2.6855e-05, Val Loss: 2.4414e-03, LR: 2.5000e-05, Time: 122.07s
2025-06-13 17:15:52,313 - Epoch 34, Train Loss: 2.5099e-05, Val Loss: 2.4060e-03, LR: 2.5000e-05, Time: 137.94s
2025-06-13 17:18:24,298 - Epoch 35, Train Loss: 2.3765e-05, Val Loss: 2.4640e-03, LR: 2.5000e-05, Time: 151.97s
2025-06-13 17:20:36,292 - Epoch 36, Train Loss: 2.2830e-05, Val Loss: 2.5066e-03, LR: 2.5000e-05, Time: 131.98s
2025-06-13 17:23:17,320 - Epoch 37, Train Loss: 2.1558e-05, Val Loss: 2.7024e-03, LR: 1.2500e-05, Time: 161.00s
2025-06-13 17:26:01,515 - Epoch 38, Train Loss: 1.9074e-05, Val Loss: 2.6948e-03, LR: 1.2500e-05, Time: 164.17s
2025-06-13 17:28:52,558 - Epoch 39, Train Loss: 1.8724e-05, Val Loss: 2.7321e-03, LR: 1.2500e-05, Time: 170.97s
2025-06-13 17:28:52,616 - Early stopping at epoch 39
2025-06-13 17:29:04,417 - [task0] On own task0: RMSE=4.7588e-02, MAE=3.9825e-02, R2=-2.5107
2025-06-13 17:29:18,234 - [task0] Forward on full: RMSE=1.2519e-01, MAE=1.0373e-01, R2=-1.2007
2025-06-13 17:29:18,253 - [task1] Training... (EWC=False, lambda=0.0000)
2025-06-13 17:31:08,567 - Epoch 1, Train Loss: 5.5891e-04, Val Loss: 1.1204e-03, LR: 1.0000e-04, Time: 110.31s
2025-06-13 17:32:50,974 - Epoch 2, Train Loss: 2.4196e-04, Val Loss: 1.8825e-03, LR: 1.0000e-04, Time: 102.35s
2025-06-13 17:34:39,503 - Epoch 3, Train Loss: 1.5183e-04, Val Loss: 2.3638e-03, LR: 1.0000e-04, Time: 108.51s
2025-06-13 17:36:30,714 - Epoch 4, Train Loss: 1.4381e-04, Val Loss: 4.0219e-03, LR: 1.0000e-04, Time: 111.18s
2025-06-13 17:38:51,984 - Epoch 5, Train Loss: 9.3532e-05, Val Loss: 4.3100e-03, LR: 1.0000e-04, Time: 141.24s
2025-06-13 17:41:04,615 - Epoch 6, Train Loss: 1.1612e-04, Val Loss: 3.5107e-03, LR: 1.0000e-04, Time: 132.60s
2025-06-13 17:42:49,135 - Epoch 7, Train Loss: 9.5568e-05, Val Loss: 1.4901e-03, LR: 5.0000e-05, Time: 104.50s
2025-06-13 17:44:08,830 - Epoch 8, Train Loss: 1.1874e-04, Val Loss: 3.5679e-03, LR: 5.0000e-05, Time: 79.68s
2025-06-13 17:49:11,294 - ==== Skipping Regular LSTM Training Phase ====
2025-06-13 17:49:11,294 - ==== Incremental Training Phase ====
2025-06-13 17:50:02,445 - Base train IDs: ['03', '05', '07', '27']
2025-06-13 17:50:02,454 - Base train size: 92079
2025-06-13 17:50:02,471 - Base val IDs: ['01']
2025-06-13 17:50:02,471 - Base val size: 28612
2025-06-13 17:50:02,471 - Update1 train IDs: ['21', '23', '25']
2025-06-13 17:50:02,471 - Update1 train size: 65674
2025-06-13 17:50:02,472 - Update1 val IDs: ['19']
2025-06-13 17:50:02,472 - Update1 val size: 23120
2025-06-13 17:50:02,472 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-13 17:50:02,472 - Update2 train size: 47891
2025-06-13 17:50:02,472 - Update2 val IDs: ['13']
2025-06-13 17:50:02,472 - Update2 val size: 6445
2025-06-13 17:50:06,614 - Test cell ID: 17
2025-06-13 17:50:06,614 - Test size: 22872
2025-06-13 17:50:06,614 - Test base size: 11139
2025-06-13 17:50:06,615 - Test update1 size: 6312
2025-06-13 17:50:06,615 - Test update2 size: 5421
2025-06-13 17:50:06,615 - Scaling datasets with RobustScaler...
2025-06-13 17:50:06,694 - [Scaler Base] center: [ 3.31975183  0.         27.55116667], scale: [0.20016217 1.86108033 1.05333333]
2025-06-13 17:50:06,699 - Base train scaler fitted and datasets scaled.
2025-06-13 17:50:06,727 - [Scaler Update1] center: [ 3.3375475   0.09004367 27.77316667], scale: [0.189709   2.018227   1.66283333]
2025-06-13 17:50:06,728 - Update1 train scaler fitted and datasets scaled.
2025-06-13 17:50:06,757 - [Scaler Update2] center: [ 3.342422    0.17992967 27.75      ], scale: [0.18428363 1.77955483 1.81783333]
2025-06-13 17:50:06,757 - Update2 train scaler fitted and datasets scaled.
2025-06-13 17:50:07,260 - [task0] Skipping training (already done or no data).
2025-06-13 17:50:10,462 - [task0] On own task0: RMSE=1.0490e+00, MAE=1.0487e+00, R2=-1704.8875
2025-06-13 17:50:16,266 - [task0] Forward on full: RMSE=9.7789e-01, MAE=9.7425e-01, R2=-133.2758
2025-06-13 17:50:16,266 - [task1] Loading previous best checkpoint from E:\00_Thesis\04_NNs\model\Naive_Fine_Tuning\incremental\task0\checkpoints\task0_best.pt
2025-06-13 17:50:16,313 - [task1] Training... (EWC=False, lambda=0.0000)
2025-06-13 17:53:34,700 - ==== Skipping Regular LSTM Training Phase ====
2025-06-13 17:53:34,700 - ==== Incremental Training Phase ====
2025-06-13 17:54:21,706 - Base train IDs: ['03', '05', '07', '27']
2025-06-13 17:54:21,708 - Base train size: 92079
2025-06-13 17:54:21,710 - Base val IDs: ['01']
2025-06-13 17:54:21,711 - Base val size: 28612
2025-06-13 17:54:21,711 - Update1 train IDs: ['21', '23', '25']
2025-06-13 17:54:21,711 - Update1 train size: 65674
2025-06-13 17:54:21,711 - Update1 val IDs: ['19']
2025-06-13 17:54:21,711 - Update1 val size: 23120
2025-06-13 17:54:21,711 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-13 17:54:21,711 - Update2 train size: 47891
2025-06-13 17:54:21,711 - Update2 val IDs: ['13']
2025-06-13 17:54:21,711 - Update2 val size: 6445
2025-06-13 17:54:26,065 - Test cell ID: 17
2025-06-13 17:54:26,066 - Test size: 22872
2025-06-13 17:54:26,066 - Test base size: 11139
2025-06-13 17:54:26,066 - Test update1 size: 6312
2025-06-13 17:54:26,066 - Test update2 size: 5421
2025-06-13 17:54:26,066 - Scaling datasets with RobustScaler...
2025-06-13 17:54:26,095 - [Scaler Base] center: [ 3.31975183  0.         27.55116667], scale: [0.20016217 1.86108033 1.05333333]
2025-06-13 17:54:26,097 - Base train scaler fitted and datasets scaled.
2025-06-13 17:54:26,122 - [Scaler Update1] center: [ 3.3375475   0.09004367 27.77316667], scale: [0.189709   2.018227   1.66283333]
2025-06-13 17:54:26,122 - Update1 train scaler fitted and datasets scaled.
2025-06-13 17:54:26,147 - [Scaler Update2] center: [ 3.342422    0.17992967 27.75      ], scale: [0.18428363 1.77955483 1.81783333]
2025-06-13 17:54:26,148 - Update2 train scaler fitted and datasets scaled.
2025-06-13 17:54:26,565 - [task0] Skipping training (already done or no data).
2025-06-13 17:54:29,903 - [task0] On own task0: RMSE=4.7271e-02, MAE=3.9992e-02, R2=-2.4641
2025-06-13 17:54:35,971 - [task0] Forward on full: RMSE=9.4418e-02, MAE=7.4689e-02, R2=-0.2518
2025-06-13 17:54:35,972 - [task1] Loading previous best checkpoint from E:\00_Thesis\04_NNs\model\Naive_Fine_Tuning\incremental\task0\checkpoints\task0_best.pt
2025-06-13 17:54:35,982 - [task1] Training... (EWC=False, lambda=0.0000)
2025-06-13 17:55:44,256 - Epoch 1, Train Loss: 1.9668e-03, Val Loss: 2.0044e-03, LR: 1.0000e-04, Time: 66.81s
2025-06-13 17:57:14,440 - Epoch 2, Train Loss: 1.0121e-03, Val Loss: 2.5132e-03, LR: 1.0000e-04, Time: 90.07s
2025-06-13 17:58:50,995 - Epoch 3, Train Loss: 6.1394e-04, Val Loss: 2.8031e-03, LR: 1.0000e-04, Time: 96.53s
2025-06-13 18:00:24,190 - Epoch 4, Train Loss: 5.8521e-04, Val Loss: 2.9962e-03, LR: 1.0000e-04, Time: 93.18s
2025-06-13 18:01:50,402 - Epoch 5, Train Loss: 4.7718e-04, Val Loss: 3.9281e-03, LR: 1.0000e-04, Time: 86.18s
2025-06-13 18:03:06,050 - Epoch 6, Train Loss: 4.5519e-04, Val Loss: 3.3777e-03, LR: 1.0000e-04, Time: 75.64s
2025-06-13 18:04:22,690 - Epoch 7, Train Loss: 4.2671e-04, Val Loss: 3.0505e-03, LR: 5.0000e-05, Time: 76.61s
2025-06-13 18:05:54,185 - Epoch 8, Train Loss: 3.4203e-04, Val Loss: 6.8803e-03, LR: 5.0000e-05, Time: 91.48s
2025-06-13 18:07:09,678 - Epoch 9, Train Loss: 2.9633e-04, Val Loss: 3.4218e-03, LR: 5.0000e-05, Time: 75.48s
2025-06-13 18:08:30,480 - Epoch 10, Train Loss: 2.4309e-04, Val Loss: 4.4510e-03, LR: 5.0000e-05, Time: 80.79s
2025-06-13 18:09:53,405 - Epoch 11, Train Loss: 2.2907e-04, Val Loss: 5.0883e-03, LR: 5.0000e-05, Time: 82.91s
2025-06-13 18:11:34,394 - Epoch 12, Train Loss: 2.4257e-04, Val Loss: 4.5566e-03, LR: 5.0000e-05, Time: 100.96s
2025-06-13 18:13:20,101 - Epoch 13, Train Loss: 2.0656e-04, Val Loss: 4.2424e-03, LR: 2.5000e-05, Time: 105.69s
2025-06-13 18:15:20,207 - Epoch 14, Train Loss: 1.6839e-04, Val Loss: 5.5228e-03, LR: 2.5000e-05, Time: 120.07s
2025-06-13 18:17:02,316 - Epoch 15, Train Loss: 1.7524e-04, Val Loss: 4.4683e-03, LR: 2.5000e-05, Time: 102.08s
2025-06-13 18:18:34,248 - Epoch 16, Train Loss: 1.5969e-04, Val Loss: 4.4944e-03, LR: 2.5000e-05, Time: 91.91s
2025-06-13 18:20:15,042 - Epoch 17, Train Loss: 1.5823e-04, Val Loss: 4.3619e-03, LR: 2.5000e-05, Time: 100.77s
2025-06-13 18:21:36,739 - Epoch 18, Train Loss: 1.4573e-04, Val Loss: 4.7564e-03, LR: 2.5000e-05, Time: 81.67s
2025-06-13 18:23:00,853 - Epoch 19, Train Loss: 1.4023e-04, Val Loss: 4.7287e-03, LR: 1.2500e-05, Time: 84.10s
