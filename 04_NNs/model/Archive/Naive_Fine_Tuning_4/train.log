2025-06-10 17:37:55,246 - ==== Skipping Regular LSTM Training Phase ====
2025-06-10 17:37:55,248 - ==== Incremental EWC Training Phase ====
2025-06-10 17:39:17,047 - Base train IDs: ['03', '05', '07', '27']
2025-06-10 17:39:17,049 - Base train size: 92079
2025-06-10 17:39:17,050 - Base val IDs: ['01']
2025-06-10 17:39:17,051 - Base val size: 28612
2025-06-10 17:39:17,052 - Update1 train IDs: ['21', '23', '25']
2025-06-10 17:39:17,053 - Update1 train size: 65674
2025-06-10 17:39:17,054 - Update1 val IDs: ['19']
2025-06-10 17:39:17,055 - Update1 val size: 23120
2025-06-10 17:39:17,056 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-10 17:39:17,056 - Update2 train size: 47891
2025-06-10 17:39:17,057 - Update2 val IDs: ['13']
2025-06-10 17:39:17,058 - Update2 val size: 6445
2025-06-10 17:39:24,109 - Test cell ID: 17

2025-06-10 17:39:24,670 - [task0] Training...
2025-06-10 17:41:01,149 - Epoch 1, Train Loss: 3.8182e-02, Val Loss: 3.7162e-04, LR: 1.0000e-04, Time: 77.71s
2025-06-10 17:42:19,517 - Epoch 2, Train Loss: 7.2867e-03, Val Loss: 1.0398e-03, LR: 1.0000e-04, Time: 78.27s
2025-06-10 17:44:16,594 - Epoch 3, Train Loss: 5.3097e-03, Val Loss: 3.9763e-04, LR: 1.0000e-04, Time: 116.98s
2025-06-10 17:46:13,431 - Epoch 4, Train Loss: 4.3294e-03, Val Loss: 5.3001e-04, LR: 1.0000e-04, Time: 116.71s
2025-06-10 17:48:11,644 - Epoch 5, Train Loss: 3.4420e-03, Val Loss: 4.4389e-04, LR: 1.0000e-04, Time: 118.09s
2025-06-10 17:50:05,732 - Epoch 6, Train Loss: 2.6740e-03, Val Loss: 4.3592e-04, LR: 1.0000e-04, Time: 114.02s
2025-06-10 17:51:54,795 - Epoch 7, Train Loss: 2.0609e-03, Val Loss: 4.5443e-04, LR: 5.0000e-05, Time: 108.92s
2025-06-10 17:53:45,143 - Epoch 8, Train Loss: 1.6517e-03, Val Loss: 4.0441e-04, LR: 5.0000e-05, Time: 110.22s
2025-06-10 17:55:02,138 - Epoch 9, Train Loss: 1.4034e-03, Val Loss: 4.0720e-04, LR: 5.0000e-05, Time: 76.86s
2025-06-10 17:56:19,189 - Epoch 10, Train Loss: 1.1964e-03, Val Loss: 5.0908e-04, LR: 5.0000e-05, Time: 77.03s
2025-06-10 17:57:39,929 - Epoch 11, Train Loss: 1.0292e-03, Val Loss: 4.2328e-04, LR: 5.0000e-05, Time: 80.71s
2025-06-10 17:59:02,101 - Epoch 12, Train Loss: 8.7673e-04, Val Loss: 4.3260e-04, LR: 5.0000e-05, Time: 82.11s
2025-06-10 18:00:22,594 - Epoch 13, Train Loss: 7.6261e-04, Val Loss: 4.6050e-04, LR: 2.5000e-05, Time: 80.42s
2025-06-10 18:01:39,688 - Epoch 14, Train Loss: 6.8891e-04, Val Loss: 4.3581e-04, LR: 2.5000e-05, Time: 77.04s
2025-06-10 18:02:56,854 - Epoch 15, Train Loss: 6.5224e-04, Val Loss: 4.0288e-04, LR: 2.5000e-05, Time: 77.12s
2025-06-10 18:04:13,669 - Epoch 16, Train Loss: 6.1891e-04, Val Loss: 4.0849e-04, LR: 2.5000e-05, Time: 76.78s
2025-06-10 18:05:31,063 - Epoch 17, Train Loss: 5.8846e-04, Val Loss: 4.4307e-04, LR: 2.5000e-05, Time: 77.35s
2025-06-10 18:06:48,143 - Epoch 18, Train Loss: 5.6076e-04, Val Loss: 4.2150e-04, LR: 2.5000e-05, Time: 77.04s
2025-06-10 18:08:05,390 - Epoch 19, Train Loss: 5.3125e-04, Val Loss: 4.3309e-04, LR: 1.2500e-05, Time: 77.21s
2025-06-10 18:09:21,883 - Epoch 20, Train Loss: 5.0491e-04, Val Loss: 3.9249e-04, LR: 1.2500e-05, Time: 76.47s
2025-06-10 18:10:38,343 - Epoch 21, Train Loss: 4.9213e-04, Val Loss: 3.9523e-04, LR: 1.2500e-05, Time: 76.44s
2025-06-10 18:10:38,363 - Early stopping at epoch 21
2025-06-10 18:10:38,370 - [task0] Training completed.
2025-06-10 18:10:38,371 - [task0] Consolidating EWC...
2025-06-10 18:11:43,289 - [task0] Consolidation done.
2025-06-11 08:52:44,667 - [task0] Evaluating BEST checkpoint...
2025-06-11 08:52:53,742 - [task0 FORWARD on test] RMSE: 0.0856, MAE: 0.0672, R2: -0.0342
2025-06-11 08:52:53,744 - [task0] Forward testing completed.
2025-06-10 18:12:14,927 - [task1] Loading best checkpoint from previous task task0...
2025-06-10 18:12:15,175 - [task1] Training...
2025-06-10 18:13:21,854 - Epoch 1, Train Loss: 8.6660e-03, Val Loss: 1.5009e-03, LR: 1.0000e-04, Time: 66.62s
2025-06-10 18:14:20,977 - Epoch 2, Train Loss: 6.9318e-03, Val Loss: 2.8907e-03, LR: 1.0000e-04, Time: 59.07s
2025-06-10 18:15:17,836 - Epoch 3, Train Loss: 6.0778e-03, Val Loss: 2.7186e-03, LR: 1.0000e-04, Time: 56.79s
2025-06-10 18:16:15,387 - Epoch 4, Train Loss: 5.4505e-03, Val Loss: 2.4075e-03, LR: 1.0000e-04, Time: 57.52s
2025-06-10 18:17:12,262 - Epoch 5, Train Loss: 4.7762e-03, Val Loss: 2.2439e-03, LR: 1.0000e-04, Time: 56.84s
2025-06-10 18:18:09,728 - Epoch 6, Train Loss: 4.2514e-03, Val Loss: 2.3972e-03, LR: 1.0000e-04, Time: 57.43s
2025-06-10 18:19:08,513 - Epoch 7, Train Loss: 3.8975e-03, Val Loss: 1.5687e-03, LR: 5.0000e-05, Time: 58.73s
2025-06-10 18:20:06,815 - Epoch 8, Train Loss: 3.1724e-03, Val Loss: 1.0895e-03, LR: 5.0000e-05, Time: 58.26s
2025-06-10 18:21:04,536 - Epoch 9, Train Loss: 2.9682e-03, Val Loss: 1.3469e-03, LR: 5.0000e-05, Time: 57.62s
2025-06-10 18:22:02,343 - Epoch 10, Train Loss: 2.3525e-03, Val Loss: 1.0991e-03, LR: 5.0000e-05, Time: 57.77s
2025-06-10 18:22:59,965 - Epoch 11, Train Loss: 1.9990e-03, Val Loss: 1.0377e-03, LR: 5.0000e-05, Time: 57.57s
2025-06-10 18:23:57,692 - Epoch 12, Train Loss: 1.7310e-03, Val Loss: 1.2923e-03, LR: 5.0000e-05, Time: 57.63s
2025-06-10 18:24:55,159 - Epoch 13, Train Loss: 1.5674e-03, Val Loss: 1.7038e-03, LR: 5.0000e-05, Time: 57.43s
2025-06-10 18:25:52,540 - Epoch 14, Train Loss: 1.3990e-03, Val Loss: 1.2375e-03, LR: 5.0000e-05, Time: 57.35s
2025-06-10 18:26:50,424 - Epoch 15, Train Loss: 1.3866e-03, Val Loss: 2.2765e-03, LR: 5.0000e-05, Time: 57.84s
2025-06-10 18:27:47,590 - Epoch 16, Train Loss: 1.1112e-03, Val Loss: 1.8602e-03, LR: 5.0000e-05, Time: 57.13s
2025-06-10 18:28:44,727 - Epoch 17, Train Loss: 1.0320e-03, Val Loss: 2.8688e-03, LR: 2.5000e-05, Time: 57.10s
2025-06-10 18:29:41,772 - Epoch 18, Train Loss: 9.2077e-04, Val Loss: 1.9693e-03, LR: 2.5000e-05, Time: 57.01s
2025-06-10 18:30:39,136 - Epoch 19, Train Loss: 8.3938e-04, Val Loss: 1.8062e-03, LR: 2.5000e-05, Time: 57.33s
2025-06-10 18:31:36,210 - Epoch 20, Train Loss: 8.0377e-04, Val Loss: 2.5814e-03, LR: 2.5000e-05, Time: 57.03s
2025-06-10 18:32:33,403 - Epoch 21, Train Loss: 7.3363e-04, Val Loss: 2.3211e-03, LR: 2.5000e-05, Time: 57.16s
2025-06-10 18:33:30,356 - Epoch 22, Train Loss: 7.0507e-04, Val Loss: 2.0450e-03, LR: 2.5000e-05, Time: 56.92s
2025-06-10 18:34:27,426 - Epoch 23, Train Loss: 6.6859e-04, Val Loss: 2.5347e-03, LR: 1.2500e-05, Time: 57.04s
2025-06-10 18:35:24,559 - Epoch 24, Train Loss: 6.0481e-04, Val Loss: 2.3798e-03, LR: 1.2500e-05, Time: 57.10s
2025-06-10 18:36:21,893 - Epoch 25, Train Loss: 5.7982e-04, Val Loss: 2.6975e-03, LR: 1.2500e-05, Time: 57.30s
2025-06-10 18:37:19,105 - Epoch 26, Train Loss: 5.7228e-04, Val Loss: 2.5506e-03, LR: 1.2500e-05, Time: 57.16s
2025-06-10 18:38:16,453 - Epoch 27, Train Loss: 5.5331e-04, Val Loss: 2.3085e-03, LR: 1.2500e-05, Time: 57.31s
2025-06-10 18:39:15,705 - Epoch 28, Train Loss: 5.3946e-04, Val Loss: 2.2444e-03, LR: 1.2500e-05, Time: 59.22s
2025-06-10 18:40:14,882 - Epoch 29, Train Loss: 5.2919e-04, Val Loss: 2.3047e-03, LR: 6.2500e-06, Time: 59.14s
2025-06-10 18:41:12,363 - Epoch 30, Train Loss: 5.0589e-04, Val Loss: 2.4999e-03, LR: 6.2500e-06, Time: 57.43s
2025-06-10 18:42:09,540 - Epoch 31, Train Loss: 4.9655e-04, Val Loss: 2.3339e-03, LR: 6.2500e-06, Time: 57.14s
2025-06-10 18:42:09,571 - Early stopping at epoch 31
2025-06-10 18:42:09,578 - [task1] Training completed.
2025-06-10 18:42:09,579 - [task1] Consolidating EWC...
2025-06-10 18:42:55,830 - [task1] Consolidation done.
2025-06-10 18:42:55,832 - [task1] Evaluating best checkpoint...
2025-06-11 08:52:53,810 - [task1] Backward testing on task0...
2025-06-11 08:53:04,354 - [task1 BACKWARD on task0] RMSE: 0.0193, MAE: 0.0164, R2: -0.0063
2025-06-11 08:53:04,355 - [task1] Backward tests completed.
2025-06-11 08:53:04,357 - [task1] Evaluating BEST checkpoint...
2025-06-11 08:53:12,886 - [task1 FORWARD on test] RMSE: 0.0753, MAE: 0.0588, R2: 0.1981
2025-06-11 08:53:12,888 - [task1] Forward testing completed.
2025-06-10 18:43:20,315 - [task2] Loading best checkpoint from previous task task1...
2025-06-10 18:43:20,436 - [task2] Training...
2025-06-10 18:44:01,818 - Epoch 1, Train Loss: 3.4317e-03, Val Loss: 1.9713e-03, LR: 1.0000e-04, Time: 41.35s
2025-06-10 18:44:41,812 - Epoch 2, Train Loss: 2.3507e-03, Val Loss: 2.0855e-03, LR: 1.0000e-04, Time: 39.92s
2025-06-10 18:45:22,119 - Epoch 3, Train Loss: 2.0583e-03, Val Loss: 1.1391e-03, LR: 1.0000e-04, Time: 40.26s
2025-06-10 18:46:02,957 - Epoch 4, Train Loss: 1.7304e-03, Val Loss: 1.2725e-03, LR: 1.0000e-04, Time: 40.76s
2025-06-10 18:46:42,633 - Epoch 5, Train Loss: 1.6108e-03, Val Loss: 1.2676e-03, LR: 1.0000e-04, Time: 39.64s
2025-06-10 18:47:22,282 - Epoch 6, Train Loss: 1.6227e-03, Val Loss: 1.2769e-03, LR: 1.0000e-04, Time: 39.61s
2025-06-10 18:48:01,860 - Epoch 7, Train Loss: 1.4129e-03, Val Loss: 9.6623e-04, LR: 1.0000e-04, Time: 39.54s
2025-06-10 18:48:41,981 - Epoch 8, Train Loss: 1.3033e-03, Val Loss: 1.4589e-03, LR: 1.0000e-04, Time: 40.05s
2025-06-10 18:49:22,248 - Epoch 9, Train Loss: 1.1437e-03, Val Loss: 1.2739e-03, LR: 1.0000e-04, Time: 40.23s
2025-06-10 18:50:02,009 - Epoch 10, Train Loss: 1.1242e-03, Val Loss: 1.0842e-03, LR: 1.0000e-04, Time: 39.72s
2025-06-10 18:50:52,226 - Epoch 11, Train Loss: 9.9695e-04, Val Loss: 1.3443e-03, LR: 1.0000e-04, Time: 50.18s
2025-06-10 18:51:42,346 - Epoch 12, Train Loss: 1.0233e-03, Val Loss: 1.2090e-03, LR: 1.0000e-04, Time: 50.04s
2025-06-10 18:52:24,794 - Epoch 13, Train Loss: 9.8294e-04, Val Loss: 1.1186e-03, LR: 5.0000e-05, Time: 42.40s
2025-06-10 18:53:06,469 - Epoch 14, Train Loss: 7.6414e-04, Val Loss: 1.5807e-03, LR: 5.0000e-05, Time: 41.63s
2025-06-10 18:53:47,173 - Epoch 15, Train Loss: 7.8561e-04, Val Loss: 8.7686e-04, LR: 5.0000e-05, Time: 40.64s
2025-06-10 18:54:40,954 - Epoch 16, Train Loss: 7.7907e-04, Val Loss: 1.3955e-03, LR: 5.0000e-05, Time: 53.66s
2025-06-10 18:55:23,844 - Epoch 17, Train Loss: 6.3991e-04, Val Loss: 8.4190e-04, LR: 5.0000e-05, Time: 42.82s
2025-06-10 18:56:05,620 - Epoch 18, Train Loss: 6.7744e-04, Val Loss: 2.1698e-03, LR: 5.0000e-05, Time: 41.65s
2025-06-10 18:56:44,967 - Epoch 19, Train Loss: 6.4663e-04, Val Loss: 8.8381e-04, LR: 5.0000e-05, Time: 39.31s
2025-06-10 18:57:23,998 - Epoch 20, Train Loss: 6.9031e-04, Val Loss: 7.6060e-04, LR: 5.0000e-05, Time: 38.99s
2025-06-10 18:58:03,641 - Epoch 21, Train Loss: 6.6477e-04, Val Loss: 9.8057e-04, LR: 5.0000e-05, Time: 39.57s
2025-06-10 18:58:42,926 - Epoch 22, Train Loss: 7.1821e-04, Val Loss: 1.0975e-03, LR: 5.0000e-05, Time: 39.24s
2025-06-10 18:59:22,923 - Epoch 23, Train Loss: 6.5860e-04, Val Loss: 1.0544e-03, LR: 5.0000e-05, Time: 39.96s
2025-06-10 19:00:03,693 - Epoch 24, Train Loss: 6.0568e-04, Val Loss: 9.5641e-04, LR: 5.0000e-05, Time: 40.72s
2025-06-10 19:00:44,691 - Epoch 25, Train Loss: 5.7539e-04, Val Loss: 1.0059e-03, LR: 5.0000e-05, Time: 40.95s
2025-06-10 19:01:25,457 - Epoch 26, Train Loss: 5.0253e-04, Val Loss: 8.0808e-04, LR: 2.5000e-05, Time: 40.72s
2025-06-10 19:02:08,354 - Epoch 27, Train Loss: 4.4832e-04, Val Loss: 6.6655e-04, LR: 2.5000e-05, Time: 42.85s
2025-06-10 19:02:52,077 - Epoch 28, Train Loss: 4.2741e-04, Val Loss: 8.5974e-04, LR: 2.5000e-05, Time: 43.60s
2025-06-10 19:03:31,503 - Epoch 29, Train Loss: 4.1069e-04, Val Loss: 8.2456e-04, LR: 2.5000e-05, Time: 39.38s
2025-06-10 19:04:11,431 - Epoch 30, Train Loss: 3.9162e-04, Val Loss: 7.8083e-04, LR: 2.5000e-05, Time: 39.89s
2025-06-10 19:04:50,684 - Epoch 31, Train Loss: 3.6760e-04, Val Loss: 6.6563e-04, LR: 2.5000e-05, Time: 39.21s
2025-06-10 19:05:30,736 - Epoch 32, Train Loss: 3.6078e-04, Val Loss: 8.8087e-04, LR: 2.5000e-05, Time: 39.95s
2025-06-10 19:06:10,980 - Epoch 33, Train Loss: 3.5369e-04, Val Loss: 6.9779e-04, LR: 2.5000e-05, Time: 40.20s
2025-06-10 19:06:50,746 - Epoch 34, Train Loss: 3.3599e-04, Val Loss: 6.6223e-04, LR: 2.5000e-05, Time: 39.69s
2025-06-10 19:07:35,021 - Epoch 35, Train Loss: 3.2112e-04, Val Loss: 6.4593e-04, LR: 2.5000e-05, Time: 44.15s
2025-06-10 19:08:14,914 - Epoch 36, Train Loss: 3.2135e-04, Val Loss: 6.8619e-04, LR: 2.5000e-05, Time: 39.82s
2025-06-10 19:08:56,730 - Epoch 37, Train Loss: 3.0580e-04, Val Loss: 6.9841e-04, LR: 2.5000e-05, Time: 41.76s
2025-06-10 19:09:38,299 - Epoch 38, Train Loss: 2.9600e-04, Val Loss: 7.4253e-04, LR: 2.5000e-05, Time: 41.51s
2025-06-10 19:10:26,365 - Epoch 39, Train Loss: 3.1059e-04, Val Loss: 7.0892e-04, LR: 2.5000e-05, Time: 48.01s
2025-06-10 19:11:30,181 - Epoch 40, Train Loss: 2.8496e-04, Val Loss: 6.6533e-04, LR: 2.5000e-05, Time: 63.68s
2025-06-10 19:12:13,588 - Epoch 41, Train Loss: 2.8181e-04, Val Loss: 4.5707e-04, LR: 2.5000e-05, Time: 43.28s
2025-06-10 19:12:53,847 - Epoch 42, Train Loss: 2.7416e-04, Val Loss: 5.0973e-04, LR: 2.5000e-05, Time: 40.15s
2025-06-10 19:13:35,204 - Epoch 43, Train Loss: 2.6805e-04, Val Loss: 6.5116e-04, LR: 2.5000e-05, Time: 41.31s
2025-06-10 19:14:14,954 - Epoch 44, Train Loss: 2.6379e-04, Val Loss: 6.9207e-04, LR: 2.5000e-05, Time: 39.70s
2025-06-10 19:14:55,778 - Epoch 45, Train Loss: 2.6297e-04, Val Loss: 7.4060e-04, LR: 2.5000e-05, Time: 40.78s
2025-06-10 19:15:36,108 - Epoch 46, Train Loss: 2.5689e-04, Val Loss: 3.4189e-04, LR: 2.5000e-05, Time: 40.28s
2025-06-10 19:16:15,534 - Epoch 47, Train Loss: 2.4672e-04, Val Loss: 5.3208e-04, LR: 2.5000e-05, Time: 39.33s
2025-06-10 19:16:54,911 - Epoch 48, Train Loss: 2.4745e-04, Val Loss: 4.3279e-04, LR: 2.5000e-05, Time: 39.34s
2025-06-10 19:17:35,030 - Epoch 49, Train Loss: 2.4407e-04, Val Loss: 5.1356e-04, LR: 2.5000e-05, Time: 40.08s
2025-06-10 19:18:14,552 - Epoch 50, Train Loss: 2.3836e-04, Val Loss: 4.8004e-04, LR: 2.5000e-05, Time: 39.48s
2025-06-10 19:18:54,344 - Epoch 51, Train Loss: 2.4069e-04, Val Loss: 4.6887e-04, LR: 2.5000e-05, Time: 39.75s
2025-06-10 19:19:34,338 - Epoch 52, Train Loss: 2.3532e-04, Val Loss: 4.6067e-04, LR: 1.2500e-05, Time: 39.95s
2025-06-10 19:20:14,187 - Epoch 53, Train Loss: 2.1859e-04, Val Loss: 2.6656e-04, LR: 1.2500e-05, Time: 39.80s
2025-06-10 19:20:54,116 - Epoch 54, Train Loss: 2.1141e-04, Val Loss: 2.9582e-04, LR: 1.2500e-05, Time: 39.84s
2025-06-10 19:21:34,096 - Epoch 55, Train Loss: 2.1480e-04, Val Loss: 3.1988e-04, LR: 1.2500e-05, Time: 39.94s
2025-06-10 19:22:13,711 - Epoch 56, Train Loss: 2.1283e-04, Val Loss: 4.0799e-04, LR: 1.2500e-05, Time: 39.57s
2025-06-10 19:23:00,826 - Epoch 57, Train Loss: 2.1015e-04, Val Loss: 2.3690e-04, LR: 1.2500e-05, Time: 47.07s
2025-06-10 19:23:51,337 - Epoch 58, Train Loss: 2.0880e-04, Val Loss: 3.1841e-04, LR: 1.2500e-05, Time: 50.36s
2025-06-10 19:24:40,395 - Epoch 59, Train Loss: 2.0535e-04, Val Loss: 3.4485e-04, LR: 1.2500e-05, Time: 48.98s
2025-06-10 19:25:27,477 - Epoch 60, Train Loss: 2.0496e-04, Val Loss: 3.4645e-04, LR: 1.2500e-05, Time: 47.01s
2025-06-10 19:26:16,501 - Epoch 61, Train Loss: 1.9934e-04, Val Loss: 3.1433e-04, LR: 1.2500e-05, Time: 48.97s
2025-06-10 19:27:01,445 - Epoch 62, Train Loss: 2.0452e-04, Val Loss: 2.8872e-04, LR: 1.2500e-05, Time: 44.88s
2025-06-10 19:27:51,127 - Epoch 63, Train Loss: 2.0313e-04, Val Loss: 2.9808e-04, LR: 6.2500e-06, Time: 49.62s
2025-06-10 19:28:35,959 - Epoch 64, Train Loss: 1.9494e-04, Val Loss: 3.0129e-04, LR: 6.2500e-06, Time: 44.77s
2025-06-10 19:29:25,995 - Epoch 65, Train Loss: 1.9129e-04, Val Loss: 2.7541e-04, LR: 6.2500e-06, Time: 49.98s
2025-06-10 19:30:11,175 - Epoch 66, Train Loss: 1.9398e-04, Val Loss: 2.9321e-04, LR: 6.2500e-06, Time: 45.11s
2025-06-10 19:31:01,784 - Epoch 67, Train Loss: 1.8869e-04, Val Loss: 3.0151e-04, LR: 6.2500e-06, Time: 50.54s
2025-06-10 19:31:50,638 - Epoch 68, Train Loss: 1.9052e-04, Val Loss: 2.8367e-04, LR: 6.2500e-06, Time: 48.78s
2025-06-10 19:32:38,747 - Epoch 69, Train Loss: 1.8912e-04, Val Loss: 2.7046e-04, LR: 3.1250e-06, Time: 48.01s
2025-06-10 19:33:23,532 - Epoch 70, Train Loss: 1.8811e-04, Val Loss: 2.7629e-04, LR: 3.1250e-06, Time: 44.71s
2025-06-10 19:34:08,758 - Epoch 71, Train Loss: 1.8412e-04, Val Loss: 2.7737e-04, LR: 3.1250e-06, Time: 45.16s
2025-06-10 19:34:52,798 - Epoch 72, Train Loss: 1.8545e-04, Val Loss: 2.7995e-04, LR: 3.1250e-06, Time: 43.97s
2025-06-10 19:35:35,619 - Epoch 73, Train Loss: 1.8683e-04, Val Loss: 2.6139e-04, LR: 3.1250e-06, Time: 42.77s
2025-06-10 19:36:17,965 - Epoch 74, Train Loss: 1.8324e-04, Val Loss: 2.8205e-04, LR: 3.1250e-06, Time: 42.30s
2025-06-10 19:37:00,756 - Epoch 75, Train Loss: 1.8160e-04, Val Loss: 3.1145e-04, LR: 1.5625e-06, Time: 42.74s
2025-06-10 19:37:42,206 - Epoch 76, Train Loss: 1.8219e-04, Val Loss: 2.6441e-04, LR: 1.5625e-06, Time: 41.40s
2025-06-10 19:38:22,190 - Epoch 77, Train Loss: 1.8262e-04, Val Loss: 2.4291e-04, LR: 1.5625e-06, Time: 39.92s
2025-06-10 19:38:22,228 - Early stopping at epoch 77
2025-06-10 19:38:22,234 - [task2] Training completed.
2025-06-10 19:38:22,236 - [task2] Consolidating EWC...
2025-06-10 19:38:56,361 - [task2] Consolidation done.
2025-06-11 08:53:12,972 - [task2] Backward testing on task0...
2025-06-11 08:53:23,388 - [task2 BACKWARD on task0] RMSE: 0.0193, MAE: 0.0164, R2: -0.0063
2025-06-11 08:53:23,391 - [task2] Backward testing on task1...
2025-06-11 08:53:31,878 - [task2 BACKWARD on task1] RMSE: 0.0322, MAE: 0.0264, R2: 0.3439
2025-06-11 08:53:31,879 - [task2] Backward tests completed.
2025-06-11 08:53:31,881 - [task2] Evaluating BEST checkpoint...
2025-06-11 08:53:40,570 - [task2 FORWARD on test] RMSE: 0.0302, MAE: 0.0248, R2: 0.8715
2025-06-11 08:53:40,572 - [task2] Forward testing completed.
2025-06-10 19:39:20,979 - ==== All tasks completed ====
2025-06-11 17:45:14,305 - ==== Skipping Regular LSTM Training Phase ====
2025-06-11 17:45:14,320 - ==== Incremental EWC Training Phase ====
2025-06-11 17:46:03,702 - Base train IDs: ['03', '05', '07', '27']
2025-06-11 17:46:03,724 - Base train size: 92079
2025-06-11 17:46:03,739 - Base val IDs: ['01']
2025-06-11 17:46:03,739 - Base val size: 28612
2025-06-11 17:46:03,739 - Update1 train IDs: ['21', '23', '25']
2025-06-11 17:46:03,739 - Update1 train size: 65674
2025-06-11 17:46:03,739 - Update1 val IDs: ['19']
2025-06-11 17:46:03,739 - Update1 val size: 23120
2025-06-11 17:46:03,739 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-11 17:46:03,739 - Update2 train size: 47891
2025-06-11 17:46:03,739 - Update2 val IDs: ['13']
2025-06-11 17:46:03,739 - Update2 val size: 6445
2025-06-11 17:46:08,323 - Test cell ID: 17
2025-06-11 17:46:08,324 - Test size: 22872
2025-06-11 17:46:08,324 - Test base size: 11139
2025-06-11 17:46:08,324 - Test update1 size: 6312
2025-06-11 17:46:08,324 - Test update2 size: 5421
2025-06-11 17:46:08,859 - [task0] Skipping training (already done or no data).
2025-06-11 17:46:08,859 - [task0] No EWC consolidation needed or already done.
2025-06-11 17:46:08,860 - [task0] Baseline test on own subset test_base...
2025-06-11 17:46:12,402 - [task0 ON test_base] RMSE: 5.1834e-02, MAE: 4.5294e-02
2025-06-11 17:46:12,402 - [task0] Evaluating BEST checkpoint...
2025-06-11 17:46:20,245 - [task0 FORWARD on test] RMSE: 8.5567e-02, MAE: 6.7164e-02, R2: -0.0342
2025-06-11 17:46:20,245 - [task0] Forward testing completed.
2025-06-11 17:46:20,246 - [task1] Loading best checkpoint from previous task task0...
2025-06-11 17:46:20,272 - [task1] Skipping training (already done or no data).
2025-06-11 17:46:20,273 - [task1] No EWC consolidation needed or already done.
2025-06-11 17:46:20,273 - [task1] Backward testing on task0...
2025-06-11 17:46:23,691 - [task1 BACKWARD on task0] RMSE: 5.1834e-02, MAE: 4.5294e-02
2025-06-11 17:46:23,691 - [task1] Backward tests completed.
2025-06-11 17:46:23,691 - [task1] Baseline test on own subset test_update1...
2025-06-11 17:46:25,402 - [task1 ON test_update1] RMSE: 7.5954e-02, MAE: 6.9640e-02
2025-06-11 17:46:25,404 - [task1] Evaluating BEST checkpoint...
2025-06-11 17:46:33,128 - [task1 FORWARD on test] RMSE: 7.5345e-02, MAE: 5.8802e-02, R2: 0.1981
2025-06-11 17:46:33,129 - [task1] Forward testing completed.
2025-06-11 17:46:33,129 - [task2] Loading best checkpoint from previous task task1...
2025-06-11 17:46:33,189 - [task2] Skipping training (already done or no data).
2025-06-11 17:46:33,190 - [task2] No EWC consolidation needed or already done.
2025-06-11 17:46:33,190 - [task2] Backward testing on task0...
2025-06-11 17:46:37,419 - [task2 BACKWARD on task0] RMSE: 5.1834e-02, MAE: 4.5294e-02
2025-06-11 17:46:37,419 - [task2] Backward testing on task1...
2025-06-11 17:46:39,650 - [task2 BACKWARD on task1] RMSE: 7.5954e-02, MAE: 6.9640e-02
2025-06-11 17:46:39,650 - [task2] Backward tests completed.
2025-06-11 17:46:39,651 - [task2] Baseline test on own subset test_update2...
2025-06-11 17:46:41,398 - [task2 ON test_update2] RMSE: 4.2017e-02, MAE: 3.6443e-02
2025-06-11 17:46:41,399 - [task2] Evaluating BEST checkpoint...
2025-06-11 17:46:50,258 - [task2 FORWARD on test] RMSE: 3.0160e-02, MAE: 2.4798e-02, R2: 0.8715
2025-06-11 17:46:50,258 - [task2] Forward testing completed.
2025-06-11 17:46:50,259 - ==== All tasks completed ====

2025-06-11 20:29:01,786 - ==== Skipping Regular LSTM Training Phase ====
2025-06-11 20:29:01,786 - ==== Incremental EWC Training Phase ====
2025-06-11 20:29:50,084 - Base train IDs: ['03', '05', '07', '27']
2025-06-11 20:29:50,100 - Base train size: 92079
2025-06-11 20:29:50,102 - Base val IDs: ['01']
2025-06-11 20:29:50,103 - Base val size: 28612
2025-06-11 20:29:50,103 - Update1 train IDs: ['21', '23', '25']
2025-06-11 20:29:50,103 - Update1 train size: 65674
2025-06-11 20:29:50,103 - Update1 val IDs: ['19']
2025-06-11 20:29:50,103 - Update1 val size: 23120
2025-06-11 20:29:50,103 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-11 20:29:50,103 - Update2 train size: 47891
2025-06-11 20:29:50,103 - Update2 val IDs: ['13']
2025-06-11 20:29:50,103 - Update2 val size: 6445
2025-06-11 20:29:54,583 - Test cell ID: 17
2025-06-11 20:29:54,583 - Test size: 22872
2025-06-11 20:29:54,583 - Test base size: 11139
2025-06-11 20:29:54,583 - Test update1 size: 6312
2025-06-11 20:29:54,583 - Test update2 size: 5421
2025-06-11 20:29:54,982 - [task0] Skipping training (already done or no data).
2025-06-11 20:29:54,982 - [task0] No EWC consolidation needed or already done.
2025-06-11 20:29:54,982 - [task0] Baseline test on own subset test_base...
2025-06-11 20:29:58,379 - [task0 ON test_base] RMSE: 5.1834e-02, MAE: 4.5294e-02
2025-06-11 20:29:58,379 - [task0] Evaluating BEST checkpoint...
2025-06-11 20:30:05,833 - [task0 FORWARD on test] RMSE: 8.5567e-02, MAE: 6.7164e-02, R2: -0.0342
2025-06-11 20:30:05,834 - [task0] Forward testing completed.
2025-06-11 20:30:05,834 - [task1] Loading best checkpoint from previous task task0...
2025-06-11 20:30:05,855 - [task1] Skipping training (already done or no data).
2025-06-11 20:30:05,855 - [task1] No EWC consolidation needed or already done.
2025-06-11 20:30:05,856 - [task1] Backward testing on task0...
2025-06-11 20:30:09,205 - [task1 BACKWARD on task0] RMSE: 5.1834e-02, MAE: 4.5294e-02
2025-06-11 20:30:09,205 - [task1] Backward tests completed.
2025-06-11 20:30:09,205 - [task1] Baseline test on own subset test_update1...
2025-06-11 20:30:10,861 - [task1 ON test_update1] RMSE: 7.5954e-02, MAE: 6.9640e-02
2025-06-11 20:30:10,861 - [task1] Evaluating BEST checkpoint...
2025-06-11 20:30:17,678 - [task1 FORWARD on test] RMSE: 7.5345e-02, MAE: 5.8802e-02, R2: 0.1981
2025-06-11 20:30:17,678 - [task1] Forward testing completed.
2025-06-11 20:30:17,678 - [task2] Loading best checkpoint from previous task task1...
2025-06-11 20:30:17,702 - [task2] Skipping training (already done or no data).
2025-06-11 20:30:17,702 - [task2] No EWC consolidation needed or already done.
2025-06-11 20:30:17,703 - [task2] Backward testing on task0...
2025-06-11 20:30:21,174 - [task2 BACKWARD on task0] RMSE: 5.1834e-02, MAE: 4.5294e-02
2025-06-11 20:30:21,175 - [task2] Backward testing on task1...
2025-06-11 20:30:23,168 - [task2 BACKWARD on task1] RMSE: 7.5954e-02, MAE: 6.9640e-02
2025-06-11 20:30:23,169 - [task2] Backward tests completed.
2025-06-11 20:30:23,169 - [task2] Baseline test on own subset test_update2...
2025-06-11 20:30:24,692 - [task2 ON test_update2] RMSE: 4.2017e-02, MAE: 3.6443e-02
2025-06-11 20:30:24,692 - [task2] Evaluating BEST checkpoint...
2025-06-11 20:30:32,457 - [task2 FORWARD on test] RMSE: 3.0160e-02, MAE: 2.4798e-02, R2: 0.8715
2025-06-11 20:30:32,458 - [task2] Forward testing completed.
2025-06-11 20:30:32,458 - ==== All tasks completed ====
2025-06-11 20:30:32,458 - Stochastic weight averaging (SWA) fine-tuning on task2 best model...
2025-06-11 20:31:14,635 - [SWA] Epoch 1/4, Loss: 1.4622e-04, LR: 5.0000e-06
2025-06-11 20:31:57,845 - [SWA] Epoch 2/4, Loss: 6.8949e-04, LR: 5.0000e-06
2025-06-11 20:32:43,266 - [SWA] Epoch 3/4, Loss: 8.1943e-06, LR: 5.0000e-06
2025-06-11 20:33:34,882 - [SWA] Epoch 4/4, Loss: 2.1887e-04, LR: 5.0000e-06
2025-06-11 20:33:34,903 - [SWA] Fine-tuning completed
2025-06-11 20:33:45,035 - [task2 SWA on test_full] RMSE: 2.9423e-02, MAE: 2.4244e-02, R2: 0.8777
2025-06-11 21:03:54,566 - ==== Skipping Regular LSTM Training Phase ====
2025-06-11 21:03:54,566 - ==== Incremental EWC Training Phase ====
2025-06-11 21:04:43,413 - ==== Skipping Regular LSTM Training Phase ====
2025-06-11 21:04:43,413 - ==== Incremental EWC Training Phase ====
2025-06-11 21:05:30,802 - Base train IDs: ['03', '05', '07', '27']
2025-06-11 21:05:30,804 - Base train size: 92079
2025-06-11 21:05:30,806 - Base val IDs: ['01']
2025-06-11 21:05:30,806 - Base val size: 28612
2025-06-11 21:05:30,806 - Update1 train IDs: ['21', '23', '25']
2025-06-11 21:05:30,806 - Update1 train size: 65674
2025-06-11 21:05:30,806 - Update1 val IDs: ['19']
2025-06-11 21:05:30,806 - Update1 val size: 23120
2025-06-11 21:05:30,806 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-11 21:05:30,806 - Update2 train size: 47891
2025-06-11 21:05:30,806 - Update2 val IDs: ['13']
2025-06-11 21:05:30,806 - Update2 val size: 6445
2025-06-11 21:05:35,220 - Test cell ID: 17
2025-06-11 21:05:35,220 - Test size: 22872
2025-06-11 21:05:35,220 - Test base size: 11139
2025-06-11 21:05:35,220 - Test update1 size: 6312
2025-06-11 21:05:35,220 - Test update2 size: 5421
2025-06-11 21:05:35,616 - [task0] Skipping training (already done or no data).
2025-06-11 21:05:35,617 - [task0] running SWA micro-finetune ...
2025-06-11 21:11:31,122 - [task0] SWA micro-finetune completed
2025-06-11 21:11:31,124 - [task0] No EWC consolidation needed or already done.
2025-06-11 21:11:31,124 - [task0] Baseline test on own subset test_base...
2025-06-11 21:11:38,163 - [task0 ON test_base] RMSE: 8.6937e-02, MAE: 7.8138e-02
2025-06-11 21:11:38,174 - [task0] Evaluating swa model on full test set...
2025-06-11 21:11:59,043 - [task0 FORWARD on test] RMSE: 8.6393e-02, MAE: 7.2113e-02, R2: -0.0543
2025-06-11 21:11:59,043 - [task0] Forward testing completed.
2025-06-11 21:11:59,044 - [task1] Loading previous best SWA checkpoint from E:\00_Thesis\04_NNs\model\Naive_Fine_Tuning\incremental\task0\checkpoints\task0_swa.pt
2025-06-11 21:11:59,051 - [task1] Skipping training (already done or no data).
2025-06-11 21:11:59,051 - [task1] running SWA micro-finetune ...
2025-06-11 21:16:43,667 - [task1] SWA micro-finetune completed
2025-06-11 21:16:43,667 - [task1] No EWC consolidation needed or already done.
2025-06-11 21:16:43,668 - [task1] Backward testing on task0...
2025-06-11 21:16:49,465 - [task1 BACKWARD on task0] RMSE: 8.6937e-02, MAE: 7.8138e-02
2025-06-11 21:16:49,465 - [task1] Backward tests completed.
2025-06-11 21:16:49,466 - [task1] Baseline test on own subset test_update1...
2025-06-11 21:16:52,231 - [task1 ON test_update1] RMSE: 6.6835e-02, MAE: 6.1247e-02
2025-06-11 21:16:52,231 - [task1] Evaluating swa model on full test set...
2025-06-11 21:17:02,723 - [task1 FORWARD on test] RMSE: 7.6421e-02, MAE: 5.9866e-02, R2: 0.1750
2025-06-11 21:17:02,723 - [task1] Forward testing completed.
2025-06-11 21:17:02,723 - [task2] Loading previous best SWA checkpoint from E:\00_Thesis\04_NNs\model\Naive_Fine_Tuning\incremental\task1\checkpoints\task1_swa.pt
2025-06-11 21:17:02,731 - [task2] Skipping training (already done or no data).
2025-06-11 21:17:02,731 - [task2] running SWA micro-finetune ...
2025-06-11 21:20:20,393 - [task2] SWA micro-finetune completed
2025-06-11 21:20:20,395 - [task2] No EWC consolidation needed or already done.
2025-06-11 21:20:20,395 - [task2] Backward testing on task0...
2025-06-11 21:20:27,063 - [task2 BACKWARD on task0] RMSE: 8.6937e-02, MAE: 7.8138e-02
2025-06-11 21:20:27,065 - [task2] Backward testing on task1...
2025-06-11 21:20:29,919 - [task2 BACKWARD on task1] RMSE: 6.6835e-02, MAE: 6.1247e-02
2025-06-11 21:20:29,920 - [task2] Backward tests completed.
2025-06-11 21:20:29,921 - [task2] Baseline test on own subset test_update2...
2025-06-11 21:20:32,223 - [task2 ON test_update2] RMSE: 4.0021e-02, MAE: 3.4946e-02
2025-06-11 21:20:32,224 - [task2] Evaluating swa model on full test set...
2025-06-11 21:20:46,056 - [task2 FORWARD on test] RMSE: 2.9497e-02, MAE: 2.4449e-02, R2: 0.8771
2025-06-11 21:20:46,057 - [task2] Forward testing completed.
2025-06-11 21:20:46,057 - ==== All tasks completed ====
