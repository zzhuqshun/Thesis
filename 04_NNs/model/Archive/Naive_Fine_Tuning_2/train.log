2025-06-13 19:28:43,909 - ==== Regular LSTM Training Phase ====
2025-06-13 19:29:30,805 - Base train IDs: ['03', '05', '07', '09', '11', '15', '21', '23', '25', '27', '29']
2025-06-13 19:29:30,805 - Base train size: 205644
2025-06-13 19:29:30,805 - Base val IDs: ['01', '19', '13']
2025-06-13 19:29:30,805 - Base val size: 58177
2025-06-13 19:29:30,805 - Update1 train IDs: []
2025-06-13 19:29:30,805 - Update1 train size: 0
2025-06-13 19:29:30,805 - Update1 val IDs: []
2025-06-13 19:29:30,805 - Update1 val size: 0
2025-06-13 19:29:30,806 - Update2 train IDs: []
2025-06-13 19:29:30,806 - Update2 train size: 0
2025-06-13 19:29:30,806 - Update2 val IDs: []
2025-06-13 19:29:30,806 - Update2 val size: 0
2025-06-13 19:29:34,745 - Test cell ID: 17
2025-06-13 19:29:34,762 - Test size: 22872
2025-06-13 19:29:34,762 - Test base size: 11139
2025-06-13 19:29:34,762 - Test update1 size: 6312
2025-06-13 19:29:34,762 - Test update2 size: 5421
2025-06-13 19:29:34,762 - Scaling datasets with RobustScaler...
2025-06-13 19:29:34,803 - [Scaler Base] center: [ 3.342422    0.17992967 27.75      ], scale: [0.18428363 1.77955483 1.81783333]
2025-06-13 19:29:34,805 - Base train scaler fitted and datasets scaled on base scaler.
2025-06-13 19:29:34,821 - [Scaler Update1] center: [ 3.342422    0.17992967 27.75      ], scale: [0.18428363 1.77955483 1.81783333]
2025-06-13 19:29:34,822 - Update1 train scaler fitted and datasets scaled on update1 scaler.
2025-06-13 19:29:34,838 - [Scaler Update2] center: [ 3.342422    0.17992967 27.75      ], scale: [0.18428363 1.77955483 1.81783333]
2025-06-13 19:29:34,838 - Update2 train scaler fitted and datasets scaled on update2 scaler.
2025-06-13 19:33:07,501 - Epoch 1, Train Loss: 2.2571e-02, Val Loss: 1.8462e-03, LR: 1.0000e-04, Time: 211.17s
2025-06-13 19:37:12,245 - Epoch 2, Train Loss: 6.4996e-03, Val Loss: 1.5726e-03, LR: 1.0000e-04, Time: 244.69s
2025-06-13 19:41:13,917 - Epoch 3, Train Loss: 3.6553e-03, Val Loss: 1.0078e-03, LR: 1.0000e-04, Time: 241.62s
2025-06-13 19:45:15,649 - Epoch 4, Train Loss: 2.3249e-03, Val Loss: 6.6145e-04, LR: 1.0000e-04, Time: 241.71s
2025-06-13 19:49:19,474 - Epoch 5, Train Loss: 1.4081e-03, Val Loss: 7.0153e-04, LR: 1.0000e-04, Time: 243.78s
2025-06-13 19:53:17,959 - Epoch 6, Train Loss: 9.4946e-04, Val Loss: 1.2023e-03, LR: 1.0000e-04, Time: 238.47s
2025-06-13 19:57:18,627 - Epoch 7, Train Loss: 7.5365e-04, Val Loss: 7.5736e-04, LR: 1.0000e-04, Time: 240.66s
2025-06-13 20:01:19,810 - Epoch 8, Train Loss: 6.4309e-04, Val Loss: 5.9448e-04, LR: 1.0000e-04, Time: 241.17s
2025-06-13 20:05:26,052 - Epoch 9, Train Loss: 5.4966e-04, Val Loss: 5.4051e-04, LR: 1.0000e-04, Time: 246.19s
2025-06-13 20:09:27,307 - Epoch 10, Train Loss: 4.4609e-04, Val Loss: 6.0003e-04, LR: 1.0000e-04, Time: 241.22s
2025-06-13 20:13:30,753 - Epoch 11, Train Loss: 3.4440e-04, Val Loss: 4.5312e-04, LR: 1.0000e-04, Time: 243.43s
2025-06-13 20:17:35,640 - Epoch 12, Train Loss: 2.7255e-04, Val Loss: 3.3157e-04, LR: 1.0000e-04, Time: 244.85s
2025-06-13 20:21:40,921 - Epoch 13, Train Loss: 2.1906e-04, Val Loss: 1.8297e-04, LR: 1.0000e-04, Time: 245.25s
2025-06-13 20:25:44,848 - Epoch 14, Train Loss: 1.8130e-04, Val Loss: 1.1899e-04, LR: 1.0000e-04, Time: 243.89s
2025-06-13 20:29:51,162 - Epoch 15, Train Loss: 1.6510e-04, Val Loss: 1.9011e-04, LR: 1.0000e-04, Time: 246.28s
2025-06-13 20:33:55,147 - Epoch 16, Train Loss: 1.5215e-04, Val Loss: 1.6548e-04, LR: 1.0000e-04, Time: 243.97s
2025-06-13 20:37:59,200 - Epoch 17, Train Loss: 1.4120e-04, Val Loss: 1.6469e-04, LR: 1.0000e-04, Time: 244.03s
2025-06-13 20:42:03,977 - Epoch 18, Train Loss: 1.3561e-04, Val Loss: 1.2219e-04, LR: 1.0000e-04, Time: 244.76s
2025-06-13 20:46:05,930 - Epoch 19, Train Loss: 1.4098e-04, Val Loss: 1.2010e-04, LR: 1.0000e-04, Time: 241.93s
2025-06-13 20:50:09,511 - Epoch 20, Train Loss: 1.2716e-04, Val Loss: 1.2844e-04, LR: 5.0000e-05, Time: 243.56s
2025-06-13 20:54:12,684 - Epoch 21, Train Loss: 1.0942e-04, Val Loss: 1.2907e-04, LR: 5.0000e-05, Time: 243.14s
2025-06-13 20:58:14,586 - Epoch 22, Train Loss: 1.0682e-04, Val Loss: 1.5118e-04, LR: 5.0000e-05, Time: 241.88s
2025-06-13 21:02:17,280 - Epoch 23, Train Loss: 1.0248e-04, Val Loss: 1.6228e-04, LR: 5.0000e-05, Time: 242.68s
2025-06-13 21:06:19,132 - Epoch 24, Train Loss: 1.0241e-04, Val Loss: 1.3230e-04, LR: 5.0000e-05, Time: 241.84s
2025-06-13 21:10:22,243 - Epoch 25, Train Loss: 1.0152e-04, Val Loss: 1.4869e-04, LR: 5.0000e-05, Time: 243.09s
2025-06-13 21:14:22,533 - Epoch 26, Train Loss: 9.9916e-05, Val Loss: 1.4557e-04, LR: 2.5000e-05, Time: 240.26s
2025-06-13 21:18:22,808 - Epoch 27, Train Loss: 9.1271e-05, Val Loss: 1.7579e-04, LR: 2.5000e-05, Time: 240.26s
2025-06-13 21:22:24,819 - Epoch 28, Train Loss: 9.1030e-05, Val Loss: 1.4809e-04, LR: 2.5000e-05, Time: 241.97s
2025-06-13 21:26:22,922 - Epoch 29, Train Loss: 9.0009e-05, Val Loss: 1.4266e-04, LR: 2.5000e-05, Time: 238.09s
2025-06-13 21:30:26,363 - Epoch 30, Train Loss: 8.9270e-05, Val Loss: 1.6134e-04, LR: 2.5000e-05, Time: 243.43s
2025-06-13 21:34:30,045 - Epoch 31, Train Loss: 8.9029e-05, Val Loss: 1.6669e-04, LR: 2.5000e-05, Time: 243.66s
2025-06-13 21:38:27,054 - Epoch 32, Train Loss: 8.8500e-05, Val Loss: 1.6182e-04, LR: 1.2500e-05, Time: 236.99s
2025-06-13 21:42:27,100 - Epoch 33, Train Loss: 8.4343e-05, Val Loss: 1.5690e-04, LR: 1.2500e-05, Time: 240.03s
2025-06-13 21:46:25,896 - Epoch 34, Train Loss: 8.3282e-05, Val Loss: 1.5525e-04, LR: 1.2500e-05, Time: 238.78s
2025-06-13 21:46:25,912 - Early stopping at epoch 34
2025-06-13 21:46:39,107 - [Joint training best model predictions] RMSE: 1.5943e-02, MAE: 1.1633e-02, R2: 0.9641
2025-06-13 21:46:39,107 - ==== Incremental Training Phase ====
2025-06-13 21:47:46,115 - Base train IDs: ['03', '05', '07', '27']
2025-06-13 21:47:46,155 - Base train size: 92079
2025-06-13 21:47:46,155 - Base val IDs: ['01']
2025-06-13 21:47:46,155 - Base val size: 28612
2025-06-13 21:47:46,155 - Update1 train IDs: ['21', '23', '25']
2025-06-13 21:47:46,155 - Update1 train size: 65674
2025-06-13 21:47:46,155 - Update1 val IDs: ['19']
2025-06-13 21:47:46,155 - Update1 val size: 23120
2025-06-13 21:47:46,155 - Update2 train IDs: ['09', '11', '15', '29']
2025-06-13 21:47:46,155 - Update2 train size: 47891
2025-06-13 21:47:46,155 - Update2 val IDs: ['13']
2025-06-13 21:47:46,155 - Update2 val size: 6445
2025-06-13 21:47:50,962 - Test cell ID: 17
2025-06-13 21:47:50,963 - Test size: 22872
2025-06-13 21:47:50,964 - Test base size: 11139
2025-06-13 21:47:50,964 - Test update1 size: 6312
2025-06-13 21:47:50,964 - Test update2 size: 5421
2025-06-13 21:47:50,964 - Scaling datasets with RobustScaler...
2025-06-13 21:47:51,052 - [Scaler Base] center: [ 3.31975183  0.         27.55116667], scale: [0.20016217 1.86108033 1.05333333]
2025-06-13 21:47:51,062 - Base train scaler fitted and datasets scaled on base scaler.
2025-06-13 21:47:51,083 - [Scaler Update1] center: [ 3.3375475   0.09004367 27.77316667], scale: [0.189709   2.018227   1.66283333]
2025-06-13 21:47:51,084 - Update1 train scaler fitted and datasets scaled on update1 scaler.
2025-06-13 21:47:51,109 - [Scaler Update2] center: [ 3.342422    0.17992967 27.75      ], scale: [0.18428363 1.77955483 1.81783333]
2025-06-13 21:47:51,109 - Update2 train scaler fitted and datasets scaled on update2 scaler.
2025-06-13 21:47:51,391 - [task0] Training... (EWC=False, lambda=0.0000)
2025-06-13 21:49:21,729 - Epoch 1, Train Loss: 2.5271e-02, Val Loss: 4.2301e-04, LR: 1.0000e-04, Time: 90.31s
2025-06-13 21:51:09,683 - Epoch 2, Train Loss: 5.9483e-03, Val Loss: 5.5062e-04, LR: 1.0000e-04, Time: 107.88s
2025-06-13 21:52:55,360 - Epoch 3, Train Loss: 4.2044e-03, Val Loss: 4.0463e-04, LR: 1.0000e-04, Time: 105.63s
2025-06-13 21:54:41,241 - Epoch 4, Train Loss: 3.2205e-03, Val Loss: 8.6426e-04, LR: 1.0000e-04, Time: 105.86s
2025-06-13 21:56:27,137 - Epoch 5, Train Loss: 2.5048e-03, Val Loss: 4.0464e-04, LR: 1.0000e-04, Time: 105.89s
2025-06-13 21:58:13,093 - Epoch 6, Train Loss: 1.9047e-03, Val Loss: 3.9692e-04, LR: 1.0000e-04, Time: 105.94s
2025-06-13 21:59:57,892 - Epoch 7, Train Loss: 1.4357e-03, Val Loss: 4.2171e-04, LR: 1.0000e-04, Time: 104.77s
2025-06-13 22:01:44,920 - Epoch 8, Train Loss: 1.0837e-03, Val Loss: 4.0485e-04, LR: 1.0000e-04, Time: 107.02s
2025-06-13 22:03:29,689 - Epoch 9, Train Loss: 8.3309e-04, Val Loss: 4.2635e-04, LR: 1.0000e-04, Time: 104.75s
2025-06-13 22:05:17,779 - Epoch 10, Train Loss: 6.6162e-04, Val Loss: 4.1430e-04, LR: 1.0000e-04, Time: 108.08s
2025-06-13 22:07:04,034 - Epoch 11, Train Loss: 5.6013e-04, Val Loss: 4.2218e-04, LR: 1.0000e-04, Time: 106.23s
2025-06-13 22:08:54,033 - Epoch 12, Train Loss: 5.0534e-04, Val Loss: 4.2213e-04, LR: 5.0000e-05, Time: 109.98s
2025-06-13 22:10:41,857 - Epoch 13, Train Loss: 4.7761e-04, Val Loss: 3.9866e-04, LR: 5.0000e-05, Time: 107.80s
2025-06-13 22:12:26,971 - Epoch 14, Train Loss: 4.6276e-04, Val Loss: 4.4840e-04, LR: 5.0000e-05, Time: 105.10s
2025-06-13 22:14:13,212 - Epoch 15, Train Loss: 4.3777e-04, Val Loss: 5.0329e-04, LR: 5.0000e-05, Time: 106.23s
2025-06-13 22:16:00,309 - Epoch 16, Train Loss: 3.8355e-04, Val Loss: 4.3865e-04, LR: 5.0000e-05, Time: 107.07s
2025-06-13 22:17:49,625 - Epoch 17, Train Loss: 3.3721e-04, Val Loss: 6.3843e-04, LR: 5.0000e-05, Time: 109.30s
2025-06-13 22:19:39,091 - Epoch 18, Train Loss: 3.3828e-04, Val Loss: 7.1899e-04, LR: 2.5000e-05, Time: 109.43s
2025-06-13 22:21:26,842 - Epoch 19, Train Loss: 2.5932e-04, Val Loss: 5.9387e-04, LR: 2.5000e-05, Time: 107.73s
2025-06-13 22:23:13,877 - Epoch 20, Train Loss: 2.3893e-04, Val Loss: 6.2671e-04, LR: 2.5000e-05, Time: 107.02s
2025-06-13 22:25:01,462 - Epoch 21, Train Loss: 2.4115e-04, Val Loss: 6.1156e-04, LR: 2.5000e-05, Time: 107.56s
2025-06-13 22:26:47,617 - Epoch 22, Train Loss: 2.2298e-04, Val Loss: 7.0063e-04, LR: 2.5000e-05, Time: 106.14s
2025-06-13 22:28:36,433 - Epoch 23, Train Loss: 2.2991e-04, Val Loss: 4.5485e-04, LR: 2.5000e-05, Time: 108.79s
2025-06-13 22:30:22,842 - Epoch 24, Train Loss: 2.1821e-04, Val Loss: 5.3074e-04, LR: 1.2500e-05, Time: 106.39s
2025-06-13 22:32:09,357 - Epoch 25, Train Loss: 2.0372e-04, Val Loss: 6.9789e-04, LR: 1.2500e-05, Time: 106.49s
2025-06-13 22:33:56,891 - Epoch 26, Train Loss: 2.0176e-04, Val Loss: 6.5710e-04, LR: 1.2500e-05, Time: 107.52s
2025-06-13 22:33:56,903 - Early stopping at epoch 26
2025-06-13 22:33:57,829 - [task0] Training completed and saved.
2025-06-13 22:34:02,818 - [task0] On own task0: RMSE=5.3739e-02, MAE=4.7515e-02, R2=-3.5881
2025-06-13 22:34:12,454 - [task0] Forward on full: RMSE=9.3715e-02, MAE=7.4205e-02, R2=-0.2406
2025-06-13 22:34:12,470 - [task1] Loading previous best checkpoint from E:\00_Thesis\04_NNs\model\Naive_Fine_Tuning\incremental\task0\checkpoints\task0_best.pt
2025-06-13 22:34:12,491 - [task1] Training... (EWC=False, lambda=0.0000)
2025-06-13 22:35:28,948 - Epoch 1, Train Loss: 3.0259e-03, Val Loss: 1.5365e-03, LR: 1.0000e-04, Time: 76.46s
2025-06-13 22:36:46,291 - Epoch 2, Train Loss: 2.3428e-03, Val Loss: 1.2666e-03, LR: 1.0000e-04, Time: 77.31s
2025-06-13 22:38:03,313 - Epoch 3, Train Loss: 1.8231e-03, Val Loss: 8.7647e-04, LR: 1.0000e-04, Time: 76.98s
2025-06-13 22:39:20,305 - Epoch 4, Train Loss: 1.6075e-03, Val Loss: 1.6212e-03, LR: 1.0000e-04, Time: 76.97s
2025-06-13 22:40:38,243 - Epoch 5, Train Loss: 1.0772e-03, Val Loss: 1.2202e-03, LR: 1.0000e-04, Time: 77.92s
2025-06-13 22:41:56,565 - Epoch 6, Train Loss: 9.9113e-04, Val Loss: 2.0956e-03, LR: 1.0000e-04, Time: 78.30s
2025-06-13 22:43:17,338 - Epoch 7, Train Loss: 9.6147e-04, Val Loss: 3.7604e-03, LR: 1.0000e-04, Time: 80.75s
2025-06-13 22:44:34,447 - Epoch 8, Train Loss: 7.7473e-04, Val Loss: 2.4441e-03, LR: 1.0000e-04, Time: 77.09s
2025-06-13 22:45:52,137 - Epoch 9, Train Loss: 6.7013e-04, Val Loss: 3.6423e-03, LR: 5.0000e-05, Time: 77.67s
2025-06-13 22:47:10,923 - Epoch 10, Train Loss: 5.7811e-04, Val Loss: 3.3416e-03, LR: 5.0000e-05, Time: 78.77s
2025-06-13 22:48:28,727 - Epoch 11, Train Loss: 5.4599e-04, Val Loss: 4.5411e-03, LR: 5.0000e-05, Time: 77.79s
2025-06-13 22:49:46,078 - Epoch 12, Train Loss: 5.6600e-04, Val Loss: 3.5789e-03, LR: 5.0000e-05, Time: 77.34s
2025-06-13 22:51:03,110 - Epoch 13, Train Loss: 5.2859e-04, Val Loss: 3.8416e-03, LR: 5.0000e-05, Time: 77.02s
2025-06-13 22:52:21,500 - Epoch 14, Train Loss: 5.0472e-04, Val Loss: 3.4874e-03, LR: 5.0000e-05, Time: 78.37s
2025-06-13 22:53:39,925 - Epoch 15, Train Loss: 4.7195e-04, Val Loss: 3.6661e-03, LR: 2.5000e-05, Time: 78.41s
2025-06-13 22:54:57,327 - Epoch 16, Train Loss: 4.3267e-04, Val Loss: 3.3840e-03, LR: 2.5000e-05, Time: 77.39s
2025-06-13 22:56:14,764 - Epoch 17, Train Loss: 4.2543e-04, Val Loss: 2.8243e-03, LR: 2.5000e-05, Time: 77.42s
2025-06-13 22:57:33,340 - Epoch 18, Train Loss: 3.8464e-04, Val Loss: 4.2805e-03, LR: 2.5000e-05, Time: 78.56s
2025-06-13 22:58:50,453 - Epoch 19, Train Loss: 3.6416e-04, Val Loss: 4.9207e-03, LR: 2.5000e-05, Time: 77.09s
2025-06-13 23:00:07,487 - Epoch 20, Train Loss: 3.7705e-04, Val Loss: 7.0490e-03, LR: 2.5000e-05, Time: 77.02s
2025-06-13 23:01:24,754 - Epoch 21, Train Loss: 3.3810e-04, Val Loss: 4.2798e-03, LR: 1.2500e-05, Time: 77.25s
2025-06-13 23:02:41,754 - Epoch 22, Train Loss: 2.7229e-04, Val Loss: 4.1403e-03, LR: 1.2500e-05, Time: 76.99s
2025-06-13 23:03:59,320 - Epoch 23, Train Loss: 2.5193e-04, Val Loss: 4.1033e-03, LR: 1.2500e-05, Time: 77.55s
2025-06-13 23:03:59,337 - Early stopping at epoch 23
2025-06-13 23:03:59,672 - [task1] Training completed and saved.
2025-06-13 23:03:59,721 - [task1] Backward on task0: Scaling with RobustScaler
2025-06-13 23:04:04,488 - [task1] Backward on task0: RMSE=2.3768e-02, MAE=2.0546e-02, R2=0.1025
2025-06-13 23:04:06,896 - [task1] On own task1: RMSE=1.9976e-01, MAE=1.9813e-01, R2=-60.6843
2025-06-13 23:04:16,739 - [task1] Forward on full: RMSE=7.2101e-02, MAE=5.5395e-02, R2=0.2657
2025-06-13 23:04:16,756 - [task2] Loading previous best checkpoint from E:\00_Thesis\04_NNs\model\Naive_Fine_Tuning\incremental\task1\checkpoints\task1_best.pt
2025-06-13 23:04:16,787 - [task2] Training... (EWC=False, lambda=0.0000)
2025-06-13 23:05:08,971 - Epoch 1, Train Loss: 4.3060e-03, Val Loss: 5.0352e-03, LR: 1.0000e-04, Time: 52.18s
2025-06-13 23:06:01,088 - Epoch 2, Train Loss: 2.2666e-03, Val Loss: 5.8577e-03, LR: 1.0000e-04, Time: 52.08s
2025-06-13 23:06:52,934 - Epoch 3, Train Loss: 1.8016e-03, Val Loss: 4.4727e-03, LR: 1.0000e-04, Time: 51.82s
2025-06-13 23:07:56,200 - Epoch 4, Train Loss: 1.4598e-03, Val Loss: 4.5644e-03, LR: 1.0000e-04, Time: 63.23s
2025-06-13 23:08:54,874 - Epoch 5, Train Loss: 1.3624e-03, Val Loss: 3.8412e-03, LR: 1.0000e-04, Time: 58.64s
2025-06-13 23:09:47,214 - Epoch 6, Train Loss: 1.2634e-03, Val Loss: 2.9306e-03, LR: 1.0000e-04, Time: 52.29s
2025-06-13 23:10:40,875 - Epoch 7, Train Loss: 1.5275e-03, Val Loss: 2.0954e-03, LR: 1.0000e-04, Time: 53.62s
2025-06-13 23:11:33,580 - Epoch 8, Train Loss: 1.0801e-03, Val Loss: 1.1897e-03, LR: 1.0000e-04, Time: 52.68s
2025-06-13 23:12:27,228 - Epoch 9, Train Loss: 8.2156e-04, Val Loss: 1.5462e-03, LR: 1.0000e-04, Time: 53.61s
2025-06-13 23:13:18,795 - Epoch 10, Train Loss: 7.6665e-04, Val Loss: 7.9050e-04, LR: 1.0000e-04, Time: 51.56s
2025-06-13 23:14:09,405 - Epoch 11, Train Loss: 6.1282e-04, Val Loss: 5.8935e-04, LR: 1.0000e-04, Time: 50.59s
2025-06-13 23:15:00,768 - Epoch 12, Train Loss: 5.4306e-04, Val Loss: 5.1138e-04, LR: 1.0000e-04, Time: 51.33s
2025-06-13 23:15:53,964 - Epoch 13, Train Loss: 5.0508e-04, Val Loss: 5.3158e-04, LR: 1.0000e-04, Time: 53.17s
2025-06-13 23:16:47,012 - Epoch 14, Train Loss: 4.6846e-04, Val Loss: 9.1715e-04, LR: 1.0000e-04, Time: 53.04s
2025-06-13 23:17:38,788 - Epoch 15, Train Loss: 4.2967e-04, Val Loss: 9.8179e-04, LR: 1.0000e-04, Time: 51.77s
2025-06-13 23:18:29,552 - Epoch 16, Train Loss: 3.9866e-04, Val Loss: 1.3092e-03, LR: 1.0000e-04, Time: 50.75s
2025-06-13 23:19:22,363 - Epoch 17, Train Loss: 3.9297e-04, Val Loss: 1.0519e-03, LR: 1.0000e-04, Time: 52.80s
2025-06-13 23:20:13,039 - Epoch 18, Train Loss: 3.7578e-04, Val Loss: 7.8750e-04, LR: 5.0000e-05, Time: 50.67s
2025-06-13 23:21:04,232 - Epoch 19, Train Loss: 3.2708e-04, Val Loss: 1.0625e-03, LR: 5.0000e-05, Time: 51.18s
2025-06-13 23:21:54,234 - Epoch 20, Train Loss: 3.1942e-04, Val Loss: 7.2317e-04, LR: 5.0000e-05, Time: 49.99s
2025-06-13 23:22:45,427 - Epoch 21, Train Loss: 3.1594e-04, Val Loss: 1.0780e-03, LR: 5.0000e-05, Time: 51.18s
2025-06-13 23:23:34,881 - Epoch 22, Train Loss: 3.0746e-04, Val Loss: 1.3008e-03, LR: 5.0000e-05, Time: 49.44s
2025-06-13 23:24:26,979 - Epoch 23, Train Loss: 3.0462e-04, Val Loss: 1.3196e-03, LR: 5.0000e-05, Time: 52.09s
2025-06-13 23:25:18,524 - Epoch 24, Train Loss: 3.0001e-04, Val Loss: 1.1787e-03, LR: 2.5000e-05, Time: 51.54s
2025-06-13 23:26:09,578 - Epoch 25, Train Loss: 2.7159e-04, Val Loss: 1.0290e-03, LR: 2.5000e-05, Time: 51.04s
2025-06-13 23:27:01,011 - Epoch 26, Train Loss: 2.6536e-04, Val Loss: 1.0861e-03, LR: 2.5000e-05, Time: 51.42s
2025-06-13 23:27:51,684 - Epoch 27, Train Loss: 2.6215e-04, Val Loss: 1.1898e-03, LR: 2.5000e-05, Time: 50.66s
2025-06-13 23:28:42,784 - Epoch 28, Train Loss: 2.5870e-04, Val Loss: 1.1873e-03, LR: 2.5000e-05, Time: 51.09s
2025-06-13 23:29:35,160 - Epoch 29, Train Loss: 2.5353e-04, Val Loss: 1.1010e-03, LR: 2.5000e-05, Time: 52.36s
2025-06-13 23:30:25,659 - Epoch 30, Train Loss: 2.4828e-04, Val Loss: 1.5727e-03, LR: 1.2500e-05, Time: 50.49s
2025-06-13 23:31:16,782 - Epoch 31, Train Loss: 2.3540e-04, Val Loss: 1.1859e-03, LR: 1.2500e-05, Time: 51.11s
2025-06-13 23:32:06,987 - Epoch 32, Train Loss: 2.3551e-04, Val Loss: 1.2419e-03, LR: 1.2500e-05, Time: 50.19s
2025-06-13 23:32:06,999 - Early stopping at epoch 32
2025-06-13 23:32:07,263 - [task2] Training completed and saved.
2025-06-13 23:32:07,323 - [task2] Backward on task0: Scaling with RobustScaler
2025-06-13 23:32:12,252 - [task2] Backward on task0: RMSE=1.8173e-02, MAE=1.5065e-02, R2=0.4753
2025-06-13 23:32:12,255 - [task2] Backward on task1: Scaling with RobustScaler
2025-06-13 23:32:14,676 - [task2] Backward on task1: RMSE=5.4936e-02, MAE=5.3263e-02, R2=-3.6654
2025-06-13 23:32:16,725 - [task2] On own task2: RMSE=9.2394e-02, MAE=9.0712e-02, R2=-25.8846
2025-06-13 23:32:26,271 - [task2] Forward on full: RMSE=5.5676e-02, MAE=4.4188e-02, R2=0.5621
2025-06-13 23:32:26,310 - Saved incremental_summary.csv.
