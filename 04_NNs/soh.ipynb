{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:   0%|          | 0/15 [00:00<?, ?cell/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:   7%|▋         | 1/15 [00:10<02:24, 10.36s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C03 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  13%|█▎        | 2/15 [00:19<02:07,  9.82s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C05 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  20%|██        | 3/15 [00:28<01:50,  9.23s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C07 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  27%|██▋       | 4/15 [00:36<01:36,  8.81s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C09 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  33%|███▎      | 5/15 [00:40<01:11,  7.16s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  40%|████      | 6/15 [00:45<00:56,  6.28s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C13 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  47%|████▋     | 7/15 [00:47<00:38,  4.85s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C15 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  53%|█████▎    | 8/15 [00:50<00:29,  4.26s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C17 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  60%|██████    | 9/15 [00:58<00:32,  5.40s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C19 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  67%|██████▋   | 10/15 [01:05<00:30,  6.07s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C21 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  73%|███████▎  | 11/15 [01:13<00:25,  6.48s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C23 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  80%|████████  | 12/15 [01:21<00:20,  6.95s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C25 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  87%|████████▋ | 13/15 [01:26<00:12,  6.46s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C27 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells:  93%|█████████▎| 14/15 [01:33<00:06,  6.70s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C29 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cells: 100%|██████████| 15/15 [01:38<00:00,  6.59s/cell]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell split completed:\n",
      "Training set: 13 cells\n",
      "Validation set: 1 cells\n",
      "Test set: 1 cells\n",
      "Final dataset sizes:\n",
      "Training set: 48609 rows (split into 13 parts)\n",
      "Validation set: 4561 rows from 1 cells\n",
      "Test set: 4602 rows from 1 cells\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data_processing import *\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score,mean_squared_error\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################################\n",
    "# Data loading\n",
    "##########################################################\n",
    "\n",
    "data_dir = \"../01_Datenaufbereitung/Output/Calculated/\"\n",
    "all_data = load_data(data_dir)\n",
    "\n",
    "train_df, val_df, test_df = split_data(all_data, train=13, val=1, test=1,parts = 1)\n",
    "train_scaled, val_scaled, test_scaled = scale_data(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Dataset definition    \n",
    "##########################################################\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df, seed_len=36, pred_len=5, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing SOH and other features\n",
    "            seed_len: Length of input sequence\n",
    "            pred_len: Length of prediction sequence\n",
    "            is_train: If True, use sliding window with stride=1, else use non-overlapping windows\n",
    "        \"\"\"\n",
    "        self.seed_len = seed_len\n",
    "        self.pred_len = pred_len\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Get unique cell IDs\n",
    "        self.cell_ids = df['cell_id'].unique()\n",
    "        \n",
    "        # Store data for each cell separately\n",
    "        self.cell_data = {}\n",
    "        self.samples = []\n",
    "        \n",
    "        for cell_id in self.cell_ids:\n",
    "            # Get data for this cell\n",
    "            cell_df = df[df['cell_id'] == cell_id]\n",
    "            # Sort by time\n",
    "            cell_df = cell_df.sort_values('Testtime[h]')\n",
    "            # Store features\n",
    "            self.cell_data[cell_id] = cell_df[['SOH_ZHU', 'Current[A]', 'Voltage[V]', 'Temperature[°C]']].values\n",
    "            \n",
    "            data_len = len(self.cell_data[cell_id])\n",
    "            \n",
    "            if is_train:\n",
    "                # Training mode: use sliding window with stride=1\n",
    "                for i in range(0, data_len - seed_len - pred_len + 1):\n",
    "                    self.samples.append({\n",
    "                        'cell_id': cell_id,\n",
    "                        'start_idx': i\n",
    "                    })\n",
    "            else:\n",
    "                # Validation/Test mode: use non-overlapping windows\n",
    "                for i in range(0, data_len - seed_len - pred_len + 1, pred_len):\n",
    "                    self.samples.append({\n",
    "                        'cell_id': cell_id,\n",
    "                        'start_idx': i\n",
    "                    })\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffle samples for training\"\"\"\n",
    "        if self.is_train:\n",
    "            random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        cell_id = sample['cell_id']\n",
    "        start_idx = sample['start_idx']\n",
    "        \n",
    "        # Get data block for this sample\n",
    "        data = self.cell_data[cell_id]\n",
    "        block = data[start_idx : start_idx + self.seed_len + self.pred_len]\n",
    "        \n",
    "        # Split into input and target sequences\n",
    "        x_seed = block[:self.seed_len]          # (seed_len, 4)\n",
    "        x_future = block[self.seed_len:]        # (pred_len, 4)\n",
    "        y_target = x_future[:, 0]               # (pred_len,)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(x_seed, dtype=torch.float32),\n",
    "            torch.tensor(x_future, dtype=torch.float32),\n",
    "            torch.tensor(y_target, dtype=torch.float32),\n",
    "            cell_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################  \n",
    "# Model definition  \n",
    "##########################################################\n",
    "class LSTMSOH(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=128, num_layers=3, dropout=0.1, pred_len=50):\n",
    "        super(LSTMSOH, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_dim, hidden_dim, \n",
    "            num_layers, batch_first=True, \n",
    "            dropout=dropout \n",
    "        )\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_dim, hidden_dim, \n",
    "            num_layers, batch_first=True, \n",
    "            dropout=dropout \n",
    "        )\n",
    "        \n",
    "        # Projection layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, future_features: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "            future_features: Future known features (batch_size, pred_len, input_dim-1)\n",
    "        Returns:\n",
    "            predictions: Predictions of shape (batch_size, pred_len)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, \n",
    "                        dtype=x.dtype, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, \n",
    "                        dtype=x.dtype, device=device)\n",
    "        \n",
    "        # Encode the input sequence\n",
    "        _, (hidden, cell) = self.encoder(x, (h0, c0))\n",
    "        \n",
    "        # Initialize decoder input with the last step of input sequence\n",
    "        decoder_input = x[:, -1:, :] # Shape: (batch_size, 1, input_dim)\n",
    "        \n",
    "        # Store predictions\n",
    "        predictions = []\n",
    "        \n",
    "        # Decode step by step\n",
    "        for t in range(self.pred_len):\n",
    "            # Get prediction for current step\n",
    "            decoder_output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            current_pred = self.fc(decoder_output[:, -1:, :]) # Shape: (batch_size, 1)\n",
    "            predictions.append(current_pred.squeeze(-1))  # Shape: (batch_size,)\n",
    "            \n",
    "            # Prepare next input\n",
    "            if future_features is not None:\n",
    "                # Combine prediction with known future features\n",
    "                next_input = torch.cat([\n",
    "                    current_pred,  # Predicted SOH for next step\n",
    "                    future_features[:, t:t+1, :]  # Known features for next step\n",
    "                ], dim=-1)\n",
    "            else:\n",
    "                # If no future features provided, use zeros\n",
    "                next_features = torch.zeros(batch_size, 1, x.size(-1)-1, device=device)\n",
    "                next_input = torch.cat([current_pred, next_features], dim=-1)\n",
    "            \n",
    "            decoder_input = next_input\n",
    "        \n",
    "        # Stack predictions along time dimension\n",
    "        predictions = torch.stack(predictions, dim=1).squeeze(-1)  # Shape: (batch_size, pred_len)\n",
    "        \n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Training process\n",
    "##########################################################      \n",
    "def evaluate_continuous(model, data, seed_len, pred_len, device):\n",
    "    \"\"\"连续预测整个序列\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        # 初始种子序列\n",
    "        current_sequence = data[:seed_len]\n",
    "        \n",
    "        # 对剩余序列进行预测\n",
    "        for i in range(seed_len, len(data) - pred_len + 1, pred_len):\n",
    "            x_seed = torch.FloatTensor(current_sequence[-seed_len:]).unsqueeze(0).to(device) # (1, seed_len, 4)\n",
    "            future_features = torch.FloatTensor(data[i:i + pred_len, 1:]).unsqueeze(0).to(device) # (1, pred_len, 3)\n",
    "            \n",
    "            # 预测下一个窗口\n",
    "            pred = model(x_seed, future_features) # (1, pred_len)\n",
    "            pred = pred.cpu().numpy().squeeze() # (pred_len,)\n",
    "            predictions.append(pred)\n",
    "            targets.append(data[i:i + pred_len, 0]) \n",
    "            \n",
    "            # 更新序列，加入预测值和实际特征\n",
    "            new_sequence = np.column_stack((\n",
    "                pred,\n",
    "                data[i:i + pred_len, 1:]\n",
    "            ))\n",
    "            current_sequence = np.vstack((current_sequence, new_sequence))\n",
    "    \n",
    "    return np.concatenate(predictions), np.concatenate(targets)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_dataset, num_epochs=10, patience=5, seed_length=36, pred_length=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], \n",
    "        'val_mae': [], 'val_rmse': [], 'val_r2': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # -----------------------------\n",
    "        # 1) Training Loop\n",
    "        # -----------------------------\n",
    "        model.train()\n",
    "        train_loader.dataset.shuffle()\n",
    "        train_losses = []\n",
    "        \n",
    "        for X_seed, X_future, Y_target, _ in train_loader:\n",
    "            X_seed = X_seed.to(device)\n",
    "            X_future = X_future.to(device)\n",
    "            Y_target = Y_target.to(device)\n",
    "            \n",
    "            future_features = X_future[:, :, 1:]\n",
    "            predictions = model(X_seed, future_features)\n",
    "            \n",
    "            loss = criterion(predictions, Y_target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "        history['train_loss'].append(mean_train_loss)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # 2) Validation Loop (自回归)\n",
    "        # -----------------------------\n",
    "        model.eval()\n",
    "        \n",
    "        # 对验证集中的电池进行连续预测\n",
    "        cell_id = val_dataset.cell_ids[0]  # 只有一个电池\n",
    "        cell_data = val_dataset.cell_data[cell_id]\n",
    "        \n",
    "        val_predictions, val_targets = evaluate_continuous(\n",
    "            model, cell_data, seed_length, pred_length, device\n",
    "        )\n",
    "        \n",
    "        # 计算验证指标\n",
    "        val_mae = mean_absolute_error(val_targets, val_predictions)\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "        val_r2 = r2_score(val_targets, val_predictions)\n",
    "        \n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {mean_train_loss:.4e}\")\n",
    "        print(f\"Val Metrics: MAE: {val_mae:.4e} | RMSE: {val_rmse:.4e} | R2: {val_r2:.4f}\")\n",
    "        \n",
    "        # Early Stopping based on MAE\n",
    "        if val_mae < best_val_loss:\n",
    "            best_val_loss = val_mae\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return history, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "Train Loss: 1.4268e-02\n",
      "Val Metrics: MAE: 4.3500e-02 | RMSE: 5.4340e-02 | R2: -0.4057\n",
      "Epoch [2/100]\n",
      "Train Loss: 1.9494e-03\n",
      "Val Metrics: MAE: 3.4646e-02 | RMSE: 4.1078e-02 | R2: 0.1967\n",
      "Epoch [3/100]\n",
      "Train Loss: 6.3074e-04\n",
      "Val Metrics: MAE: 6.0723e-02 | RMSE: 7.2105e-02 | R2: -1.4751\n",
      "Epoch [4/100]\n",
      "Train Loss: 3.9798e-04\n",
      "Val Metrics: MAE: 3.4080e-02 | RMSE: 4.0147e-02 | R2: 0.2327\n",
      "Epoch [5/100]\n",
      "Train Loss: 3.2346e-04\n",
      "Val Metrics: MAE: 3.6777e-02 | RMSE: 4.2208e-02 | R2: 0.1519\n",
      "Epoch [6/100]\n",
      "Train Loss: 2.8937e-04\n",
      "Val Metrics: MAE: 3.5765e-02 | RMSE: 4.1223e-02 | R2: 0.1910\n",
      "Epoch [7/100]\n",
      "Train Loss: 2.6699e-04\n",
      "Val Metrics: MAE: 3.5667e-02 | RMSE: 4.2691e-02 | R2: 0.1324\n",
      "Epoch [8/100]\n",
      "Train Loss: 2.4578e-04\n",
      "Val Metrics: MAE: 3.5688e-02 | RMSE: 4.3222e-02 | R2: 0.1107\n",
      "Epoch [9/100]\n",
      "Train Loss: 2.9189e-04\n",
      "Val Metrics: MAE: 3.5080e-02 | RMSE: 4.2529e-02 | R2: 0.1389\n",
      "Epoch [10/100]\n",
      "Train Loss: 3.7056e-04\n",
      "Val Metrics: MAE: 6.8496e-02 | RMSE: 7.9646e-02 | R2: -2.0198\n",
      "Epoch [11/100]\n",
      "Train Loss: 4.2552e-04\n",
      "Val Metrics: MAE: 4.1502e-02 | RMSE: 5.2305e-02 | R2: -0.3024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 36\u001b[0m\n\u001b[0;32m     29\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[0;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m     31\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m     32\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m history, best_model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_length\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model_state, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 68\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_dataset, num_epochs, patience, seed_length, pred_length)\u001b[0m\n\u001b[0;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, Y_target)\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     71\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\zzhuqshun\\.conda\\envs\\ML\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zzhuqshun\\.conda\\envs\\ML\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zzhuqshun\\.conda\\envs\\ML\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed_length = 13\n",
    "pred_length = 10\n",
    "batch_size = 16  \n",
    "hidden_dim = 80\n",
    "num_layers = 5\n",
    "dropout = 0.5\n",
    "\n",
    "# Create datasets with appropriate modes\n",
    "train_dataset = SequenceDataset(train_scaled, seed_len=seed_length, pred_len=pred_length, is_train=True)\n",
    "val_dataset = SequenceDataset(val_scaled, seed_len=seed_length, pred_len=pred_length, is_train=False)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataset = SequenceDataset(val_scaled, seed_len=seed_length, pred_len=pred_length, is_train=False)\n",
    "\n",
    "# Model initialization\n",
    "model = LSTMSOH(\n",
    "    input_dim=4,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    pred_len=pred_length\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-3\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history, best_model_state = train_model(\n",
    "    model, criterion, optimizer,\n",
    "    train_loader, val_dataset,\n",
    "    num_epochs=100,\n",
    "    patience=10,\n",
    "    seed_length=seed_length,\n",
    "    pred_length=pred_length \n",
    ")\n",
    "\n",
    "# Save best model\n",
    "torch.save(best_model_state, \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "# Evaluation\n",
    "##########################################################  \n",
    "def test_model(model, test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    # 获取测试电池数据\n",
    "    cell_id = test_dataset.cell_ids[0]  # 只有一个电池\n",
    "    cell_data = test_dataset.cell_data[cell_id]\n",
    "    \n",
    "    # 进行连续预测\n",
    "    predictions, targets = evaluate_continuous(\n",
    "        model, cell_data, seed_length, pred_length, device\n",
    "    )\n",
    "    \n",
    "    # 计算指标\n",
    "    metrics = {\n",
    "        'r2': r2_score(targets, predictions),\n",
    "        'mae': mean_absolute_error(targets, predictions),\n",
    "        'rmse': np.sqrt(mean_squared_error(targets, predictions))\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"R2: {metrics['r2']:.5f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.5e}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.5e}\")\n",
    "    \n",
    "    return predictions, targets, metrics\n",
    "# 加载最佳模型进行评估\n",
    "model.load_state_dict(best_model_state)\n",
    "test_dataset = SequenceDataset(test_scaled, seed_len=seed_length, pred_len=pred_length, is_train=False)\n",
    "\n",
    "# 评估模型\n",
    "predictions, targets, metrics = test_model(model, test_dataset)\n",
    "\n",
    "# 绘制预测结果\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(targets, label='Ground Truth')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title(f\"SOH Prediction - Test Set\\nR2: {metrics['r2']:.5f} | MAE: {metrics['mae']:.5e} | RMSE: {metrics['rmse']:.5e}\")\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('SOH')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
