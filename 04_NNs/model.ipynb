{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "from data_processing import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score,mean_squared_error\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "data_dir = \"../01_Datenaufbereitung/Output/Calculated/\"\n",
    "all_data = load_data(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_data(all_data, train=13, val=1, test=1,parts = 5)\n",
    "train_scaled, val_scaled, test_scaled = scale_data(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize data\n",
    "# visualize_data(all_data)\n",
    "# inspect_data_ranges(all_data)\n",
    "# inspect_data_ranges(train_scaled)\n",
    "# plot_dataset_soh(train_df, \"Train\")\n",
    "# plot_dataset_soh(val_df, \"Validation\")\n",
    "# plot_dataset_soh(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df, seed_len = 36, pred_len = 5):\n",
    "        self.seed_len = seed_len\n",
    "        self.pred_len = pred_len\n",
    "        self.data = df[[\"SOH_ZHU\",'Current[A]', 'Voltage[V]','Temperature[Â°C]']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.seed_len + self.pred_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # X: (batch_size, seq_len, num_features)\n",
    "        x_seq = self.data[idx : idx + self.seed_len]\n",
    "        # Y: (batch_size, pred_len)\n",
    "        y_seq = self.data[idx + self.seed_len : idx + self.seed_len + self.pred_len, 0]\n",
    "\n",
    "        x = torch.tensor(x_seq, dtype=torch.float32)\n",
    "        y = torch.tensor(y_seq, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "# Using ground truth of SOH and 3 covariances\n",
    "seq_length=72\n",
    "batch_size=32\n",
    "train_dataset = SequenceDataset(train_scaled, seed_len=seq_length)\n",
    "val_dataset = SequenceDataset(val_scaled, seed_len=seq_length)\n",
    "test_dataset = SequenceDataset(test_scaled, seed_len=seq_length)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSOH(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
    "        super(LSTMSOH, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout= dropout)\n",
    "        # Attention layer: project hidden state at each time step to a scalar attention weight\n",
    "        # self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, dtype=x.dtype, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, dtype=x.dtype, device=x.device)\n",
    "        lstm_out, _ = self.lstm(x,(h0,c0))  # lstm_out shape: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # # # Compute attention scores and normalize them\n",
    "        # attn_scores = self.attention(lstm_out)  # shape: (batch_size, seq_len, 1)\n",
    "        # attn_weights = torch.softmax(attn_scores, dim=1)  # softmax over seq_len\n",
    "        \n",
    "        # # # Compute the context vector as the weighted sum of LSTM outputs\n",
    "        # context = torch.sum(attn_weights * lstm_out, dim=1)  # shape: (batch_size, hidden_dim)\n",
    "        # out = self.fc(context)  # Final prediction, shape: (batch_size, 1)\n",
    "        \n",
    "        out = self.fc(lstm_out[:,-1,:]) # (batch_size, hidden_dim) -> (batch_size, 1)\n",
    "        \n",
    "        return out.squeeze(-1) #(batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10, patience=5):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    history = {'train_loss': [],'val_loss': [],'val_mae': [],'val_rmse': [],'val_r2': []}\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    target_idx = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # -----------------------------\n",
    "        # 1) Training Loop (pure autoregressive)\n",
    "        # -----------------------------\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for X_batch, Y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=False):\n",
    "            X_batch = X_batch.to(device)  # shape: (batch_size, seed_len, num_features)\n",
    "            Y_batch = Y_batch.to(device)  # shape: (batch_size, pred_len)\n",
    "            \n",
    "            batch_size, seed_len, num_features = X_batch.shape\n",
    "            pred_len = Y_batch.shape[1]\n",
    "            \n",
    "            # current_seq als autoregressive rolling window\n",
    "            current_seq = X_batch.clone()  # (batch_size, seed_len, num_features)\n",
    "            preds_steps = []\n",
    "            \n",
    "            for t in range(pred_len):\n",
    "                # 1) Predicting the next time step with the current sequence\n",
    "                pred = model(current_seq)  # (batch_size,)\n",
    "                preds_steps.append(pred.unsqueeze(1))  # -> (batch_size, 1)\n",
    "                \n",
    "                # 2) Write pred back to current_seq's target column, ready for the next time step.\n",
    "                # - If there are other features for the next moment, replace them with the true values from the X_batch.\n",
    "                # - Otherwise, leave the last feature unchanged.\n",
    "                if seed_len + t < X_batch.shape[1]:\n",
    "                    # Explain that X_batch also provides other characteristics of this moment in time\n",
    "                    next_input = X_batch[:, seed_len + t, :].clone()\n",
    "                else:\n",
    "                    # exceed seed_len or no more features, only the current last frame will be kept.\n",
    "                    next_input = current_seq[:, -1, :].clone()\n",
    "                \n",
    "                # Replace target columns with model predictions\n",
    "                next_input[:, target_idx] = pred\n",
    "                \n",
    "                # 3) Move the window: remove the top frame, put in a new prediction frame\n",
    "                #   current_seq[:, 1:, :] -> drop the last step\n",
    "                #   next_input.unsqueeze(1) -> (batch_size, 1, num_features)\n",
    "                current_seq = torch.cat([current_seq[:, 1:, :], next_input.unsqueeze(1)], dim=1)\n",
    "            \n",
    "            preds_steps = torch.cat(preds_steps, dim=1)\n",
    "            loss = criterion(preds_steps, Y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "        history['train_loss'].append(mean_train_loss)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # 2) Validation Loop (pure autoregressive)\n",
    "        # -----------------------------\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_val, Y_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                Y_val = Y_val.to(device)\n",
    "                batch_size, seed_len, num_features = X_val.shape\n",
    "                pred_len = Y_val.shape[1]\n",
    "                \n",
    "                current_seq = X_val.clone()\n",
    "                preds_steps = []\n",
    "                \n",
    "                for t in range(pred_len):\n",
    "                    pred = model(current_seq)  # (batch_size,)\n",
    "                    preds_steps.append(pred.unsqueeze(1))\n",
    "                    \n",
    "                    if seed_len + t < X_val.shape[1]:\n",
    "                        next_input = X_val[:, seed_len + t, :].clone()\n",
    "                    else:\n",
    "                        next_input = current_seq[:, -1, :].clone()\n",
    "                    \n",
    "                    next_input[:, target_idx] = pred\n",
    "                    current_seq = torch.cat([current_seq[:, 1:, :], next_input.unsqueeze(1)], dim=1)\n",
    "                \n",
    "                preds_steps = torch.cat(preds_steps, dim=1)  # (batch_size, pred_len)\n",
    "                val_loss = criterion(preds_steps, Y_val)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                all_preds.append(preds_steps.cpu().numpy())\n",
    "                all_targets.append(Y_val.cpu().numpy())\n",
    "        \n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        history['val_loss'].append(mean_val_loss)\n",
    "        \n",
    "        # Calculate overall MAE, RMSE, R2\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "        mae = np.mean(np.abs(all_preds - all_targets))\n",
    "        rmse = np.sqrt(np.mean((all_preds - all_targets)**2))\n",
    "        \n",
    "        ss_res = np.sum((all_targets - all_preds)**2)\n",
    "        ss_tot = np.sum((all_targets - np.mean(all_targets))**2)\n",
    "        r2 = 1 - ss_res / ss_tot if ss_tot > 1e-12 else 0.0\n",
    "        \n",
    "        history['val_mae'].append(mae)\n",
    "        history['val_rmse'].append(rmse)\n",
    "        history['val_r2'].append(r2)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {mean_train_loss:.4e} | Val Loss: {mean_val_loss:.4e} | \"\n",
    "              f\"MAE: {mae:.4e} | RMSE: {rmse:.4e} | R2: {r2:.4f}\")\n",
    "        \n",
    "        # -----------------------------\n",
    "        # 3) Early Stopping\n",
    "        # -----------------------------\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            best_val_loss = mean_val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} because validation loss did not improve.\")\n",
    "                break\n",
    "    \n",
    "    return history, best_model_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSOH(input_dim=4, hidden_dim=128, num_layers=3, dropout=0.3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "history, best_state = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=50, patience=10)\n",
    "torch.save(best_state, \"best_model.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['train_loss'], label='Train Loss')\n",
    "plt.semilogy(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 256, step = 16)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.1,0.5)\n",
    "    weight_decay= trial.suggest_float('weight_decay',1e-5,1e-1, log=True)\n",
    "    \n",
    "    seed_len = trial.suggest_int('seed_len', 12, 128)\n",
    "    pred_len = trial.suggest_int('pred_len', 1, 20)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64, step = 8)\n",
    "    \n",
    "    train_dataset = SequenceDataset(train_scaled, seed_len=seed_len, pred_len=pred_len)\n",
    "    val_dataset = SequenceDataset(val_scaled, seed_len=seed_len, pred_len=pred_len)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    # Instantiate model with suggested hyperparameters\n",
    "    model = LSTMSOH(input_dim=4, hidden_dim=hidden_size, num_layers=num_layers, dropout=dropout).type(torch.float32).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) #L2 regularization\n",
    "    history, _ = train_model(model, criterion, optimizer, train_loader, val_loader)\n",
    "\n",
    "    # Extract last validation loss\n",
    "    last_val_loss = history['val_loss'][-1]\n",
    "    return last_val_loss\n",
    "\n",
    "    # Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Extract best trial\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: {best_trial}\")\n",
    "\n",
    "best_hyperparams = study.best_trial.params\n",
    "print('Best hyperparameters:', best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### predict_autoregressive\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval()\n",
    "# Copy the data for the specified input columns, ensuring the original df is not modified\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test, Y_test in test_loader:\n",
    "        X_test = X_test.to(device)\n",
    "        Y_test = Y_test.to(device)\n",
    "        batch_size, seed_len, num_features = X_test.shape\n",
    "        pred_len = Y_test.shape[1]\n",
    "        \n",
    "        current_seq = X_test.clone()\n",
    "        preds_steps = []\n",
    "        \n",
    "        for t in range(pred_len):\n",
    "            pred = model(current_seq) \n",
    "            preds_steps.append(pred.unsqueeze(1))\n",
    "            \n",
    "            if seed_len + t < X_test.shape[1]:\n",
    "                next_input = X_test[:, seed_len + t, :].clone()\n",
    "            else:\n",
    "                next_input = current_seq[:, -1, :].clone()\n",
    "            \n",
    "            next_input[:, 0] = pred\n",
    "            current_seq = torch.cat([current_seq[:, 1:, :], next_input.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        preds_steps = torch.cat(preds_steps, dim=1)  # (batch_size, pred_len)         \n",
    "        all_preds.append(preds_steps.cpu().numpy())\n",
    "        all_targets.append(Y_test.cpu().numpy())\n",
    "        \n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "r2 = r2_score(all_targets, all_preds)\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "print(f\"R2:{r2:.5f} | MAE: {mae:.5e}| RMSE:{rmse:.5e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(all_targets[:, 0], label=\"Ground Truth SOH\")\n",
    "plt.plot(all_preds[:, 0], label=\"Predicted SOH\")\n",
    "plt.title(\"Autoregressive SOH-Estimation - Test \")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"SOH\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
