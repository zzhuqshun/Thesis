{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "%matplotlib widget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from darts.metrics import rmse, mae\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_parts(data: pd.DataFrame, parts: int = 15) -> Dict[str, pd.DataFrame]:\n",
    "    # Split data into parts\n",
    "    chunk_size = len(data) // parts\n",
    "    return {f\"{idx+1}\": data.iloc[idx * chunk_size:(idx + 1) * chunk_size] for idx in range(parts)}\n",
    "\n",
    "def load_data(data_dir: str) -> Tuple[Dict, Dict]:\n",
    "    data_path = Path(data_dir)\n",
    "    all_data = {}\n",
    "\n",
    "    # Find all parquet files\n",
    "    parquet_files = list(data_path.glob(\"**/df*.parquet\"))\n",
    "    print(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "    for file_path in tqdm(parquet_files, desc=\"Processing cells\", unit=\"cell\"):\n",
    "        # Extract cell number from parent directory name\n",
    "        file_name = file_path.stem  \n",
    "        cell_number = file_name.replace('df_', '')  \n",
    "        cell_name = f'C{cell_number}'  \n",
    "        tqdm.write(f\"Processing {cell_name} ...\")\n",
    "            \n",
    "        # Load and process data\n",
    "        data = pd.read_parquet(file_path)\n",
    "        data['Absolute_Time[yyyy-mm-dd hh:mm:ss]'] = pd.to_datetime(data['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "        \n",
    "        # Select relevant columns\n",
    "        data = data[['Absolute_Time[yyyy-mm-dd hh:mm:ss]', 'Current[A]', 'Voltage[V]', \n",
    "                    'Temperature[째C]', 'SOH_ZHU']]\n",
    "        \n",
    "        # Resample to hourly\n",
    "        data.set_index('Absolute_Time[yyyy-mm-dd hh:mm:ss]', inplace=True)\n",
    "        data_hourly = data.resample('h').mean().reset_index()\n",
    "        \n",
    "        # Fill missing values\n",
    "        data_hourly.interpolate(method='linear', inplace=True)\n",
    "        data_hourly['SOH_ZHU'] = data_hourly['SOH_ZHU'].fillna(1)\n",
    "        \n",
    "        # Convert to time series for full data\n",
    "        target_series_full = TimeSeries.from_dataframe(data_hourly, 'Absolute_Time[yyyy-mm-dd hh:mm:ss]', 'SOH_ZHU')\n",
    "        covariates_full = TimeSeries.from_dataframe(data_hourly, 'Absolute_Time[yyyy-mm-dd hh:mm:ss]', ['Current[A]', 'Voltage[V]', 'Temperature[째C]'])\n",
    "        target_series_full, covariates_full = target_series_full.slice_intersect(covariates_full), covariates_full.slice_intersect(target_series_full)\n",
    "        scaler_full = Scaler(scaler=MinMaxScaler(feature_range=(-1,1)))\n",
    "        covariates_scaled_full = scaler_full.fit_transform(covariates_full)\n",
    "        \n",
    "        all_data[cell_name] = {'target': target_series_full, 'covariates_scaled': covariates_scaled_full, 'df': data_hourly }\n",
    "\n",
    "    return all_data\n",
    "\n",
    "data_dir = \"../../01_Datenaufbereitung/Output/Calculated/\"\n",
    "all_data = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data_ranges(data_dict: dict):\n",
    "   \"\"\"\n",
    "   Inspect time ranges and value ranges for each battery in the data dictionary\n",
    "   \"\"\"\n",
    "   for cell_name, cell_data in data_dict.items():\n",
    "       print(f\"\\n=== {cell_name} ===\")\n",
    "       \n",
    "       # Get target data range\n",
    "       target = cell_data['target']\n",
    "       target_values = target.values().flatten()  # Flatten array for calculation\n",
    "       print(\"\\nTarget (SOH_ZHU):\")\n",
    "       print(f\"Time Range: {target.start_time()} to {target.end_time()}\")\n",
    "       print(f\"Value Range: {target_values.min():.4f} to {target_values.max():.4f}\")\n",
    "       print(f\"Number of Data Points: {len(target)}\")\n",
    "       \n",
    "       # Get covariates data range\n",
    "       covariates = cell_data['covariates_scaled']\n",
    "       cov_values = covariates.values()\n",
    "       print(\"\\nCovariates (scaled):\")\n",
    "       for i, feature in enumerate(covariates.components):\n",
    "           values = cov_values[:, i].flatten()\n",
    "           print(f\"{feature}:\")\n",
    "           print(f\"Value Range: {values.min():.4f} to {values.max():.4f}\")\n",
    "\n",
    "# View all data ranges\n",
    "print(\"All Data Ranges:\")\n",
    "inspect_data_ranges(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cell_data(all_data: dict, train=13, val=1, test=1, parts=15) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"Splits the dataset into training, validation, and test sets, then further divides train and val into parts.\"\"\"\n",
    "    \n",
    "    cell_names = list(all_data.keys())\n",
    "    np.random.seed(773)\n",
    "    np.random.shuffle(cell_names)\n",
    "\n",
    "    # 1. **Split data at the cell level into train/val/test sets**\n",
    "    train_cells = cell_names[:train]\n",
    "    val_cells = cell_names[train:train + val]\n",
    "    test_cells = cell_names[train + val:train + val + test]\n",
    "\n",
    "    print(f\"Cell split completed:\")\n",
    "    print(f\"Training set: {len(train_cells)} cells\")\n",
    "    print(f\"Validation set: {len(val_cells)} cells\")\n",
    "    print(f\"Test set: {len(test_cells)} cells\")\n",
    "\n",
    "    train_parts = []\n",
    "    val_parts = []\n",
    "\n",
    "    # 2. Split training data into smaller parts\n",
    "    for cell in train_cells:\n",
    "        split_data = split_data_into_parts(all_data[cell]['df'], parts=parts)\n",
    "        for part_idx, df_part in split_data.items():\n",
    "            part_name = f\"{cell}_{part_idx}\"\n",
    "            target_series_part = TimeSeries.from_dataframe(df_part, 'Absolute_Time[yyyy-mm-dd hh:mm:ss]', 'SOH_ZHU')\n",
    "            covariates_part = TimeSeries.from_dataframe(df_part, 'Absolute_Time[yyyy-mm-dd hh:mm:ss]', ['Current[A]', 'Voltage[V]', 'Temperature[째C]'])\n",
    "            target_series_part, covariates_part = target_series_part.slice_intersect(covariates_part), covariates_part.slice_intersect(target_series_part)\n",
    "            scaler_part = Scaler(scaler=MinMaxScaler(feature_range=(-1,1)))\n",
    "            covariates_scaled_part = scaler_part.fit_transform(covariates_part)\n",
    "            train_parts.append((part_name, {'target': target_series_part, 'covariates_scaled': covariates_scaled_part}))\n",
    "\n",
    "    # 3. Split validation data into smaller parts\n",
    "    for cell in val_cells:\n",
    "        split_data = split_data_into_parts(all_data[cell]['df'], parts=parts)\n",
    "        for part_idx, df_part in split_data.items():\n",
    "            part_name = f\"{cell}_{part_idx}\"\n",
    "            target_series_part = TimeSeries.from_dataframe(df_part, 'Absolute_Time[yyyy-mm-dd hh:mm:ss]', 'SOH_ZHU')\n",
    "            covariates_part = TimeSeries.from_dataframe(df_part, 'Absolute_Time[yyyy-mm-dd hh:mm:ss]', ['Current[A]', 'Voltage[V]', 'Temperature[째C]'])\n",
    "            target_series_part, covariates_part = target_series_part.slice_intersect(covariates_part), covariates_part.slice_intersect(target_series_part)\n",
    "            scaler_part = Scaler(scaler=MinMaxScaler(feature_range=(-1,1)))\n",
    "            covariates_scaled_part = scaler_part.fit_transform(covariates_part)\n",
    "            val_parts.append((part_name, {'target': target_series_part, 'covariates_scaled': covariates_scaled_part}))\n",
    "\n",
    "    # 4. **Combine and shuffle train and validation parts**\n",
    "    all_parts = train_parts + val_parts\n",
    "    np.random.shuffle(all_parts)\n",
    "\n",
    "    # 5. **Reassign train and validation parts**\n",
    "    new_train_size = train * parts\n",
    "    new_val_size = val * parts\n",
    "\n",
    "    train_data = dict(all_parts[:new_train_size])\n",
    "    val_data = dict(all_parts[new_train_size:new_train_size + new_val_size])\n",
    "\n",
    "    # 6. **Keep test data as full cells without splitting**\n",
    "    test_data = {cell: all_data[cell] for cell in test_cells}\n",
    "\n",
    "    print(f\"Data split completed:\")\n",
    "    print(f\"Training set: {len(train_data)} parts\")\n",
    "    print(f\"Validation set: {len(val_data)} parts\")\n",
    "    print(f\"Test set: {len(test_data)} full cells\")\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Execute data split\n",
    "train_data, val_data, test_data = split_cell_data(all_data)\n",
    "\n",
    "# Inspect training data\n",
    "inspect_data_ranges(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_soh(data_dict: dict, title: str, figsize=(10, 7)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot each cell's SOH\n",
    "    for cell_name, cell_data in data_dict.items():\n",
    "        target = cell_data['target']\n",
    "        plt.plot(target.time_index, target.values().flatten(), label=cell_name)\n",
    "    \n",
    "    plt.title(f'{title} Set SOH Curves')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SOH_ZHU')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all three datasets\n",
    "plot_dataset_soh(train_data, \"Training\")\n",
    "plot_dataset_soh(val_data, \"Validation\")\n",
    "plot_dataset_soh(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    targets = []\n",
    "    covariates = []\n",
    "    for cell_data in data.values():\n",
    "        targets.append(cell_data['target'])\n",
    "        covariates.append(cell_data['covariates_scaled'])\n",
    "    \n",
    "    series = targets[0]\n",
    "    cov = covariates[0]\n",
    "    for i in range(1, len(targets)):\n",
    "        series = series.concatenate(targets[i], ignore_time_axis=True)\n",
    "        cov = cov.concatenate(covariates[i], ignore_time_axis=True)\n",
    "    return series, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    # 1. Search - Basic structure\n",
    "    input_chunk_length = trial.suggest_int(\"input_chunk_length\", 24, 48, step=12)\n",
    "    output_chunk_length = trial.suggest_int(\"output_chunk_length\", 1, 24, step=12)\n",
    "    num_blocks = trial.suggest_int(\"num_blocks\", 2, 5)\n",
    "    num_stacks = trial.suggest_int(\"num_stacks\", 2, 5)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"ReLU\", \"LeakyReLU\"])\n",
    "    \n",
    "    # 2. Search - Training parameters\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    layer_widths = trial.suggest_categorical(\"layer_widths\", [128, 256, 512])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    expansion_coefficient_dim = trial.suggest_int(\"expansion_coefficient_dim\", 8, 32, step=8)\n",
    "\n",
    "    # Define and train model\n",
    "    model = NBEATSModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=output_chunk_length,\n",
    "        num_blocks=num_blocks,\n",
    "        num_stacks=num_stacks,\n",
    "        batch_size=batch_size,\n",
    "        layer_widths=layer_widths,\n",
    "        dropout=dropout_rate,\n",
    "        expansion_coefficient_dim=expansion_coefficient_dim, \n",
    "        random_state=773,\n",
    "        activation=activation,\n",
    "        pl_trainer_kwargs={\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"devices\": 1,\n",
    "        \"callbacks\": [\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1),\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\", verbose=True),  \n",
    "            ],\n",
    "        \"enable_checkpointing\": True\n",
    "        },\n",
    "        optimizer_kwargs={\"lr\": 0.0001}, \n",
    "        lr_scheduler_cls=ReduceLROnPlateau,\n",
    "        lr_scheduler_kwargs={\n",
    "            \"mode\": \"min\",  \n",
    "            \"factor\": 0.5,  \n",
    "            \"patience\": 15,  \n",
    "            \"min_lr\": 1e-6  \n",
    "    }\n",
    "    )\n",
    "    \n",
    "    train_series, train_cov = prepare_data(train_data)\n",
    "    val_series, val_cov = prepare_data(val_data)\n",
    "\n",
    "    model.fit(series=train_series, past_covariates=train_cov, \n",
    "              val_series=val_series, val_past_covariates=val_cov, epochs=100)  \n",
    "    \n",
    "    # Retrieve best validation loss directly from the training process\n",
    "    best_val_loss = model.trainer.checkpoint_callback.best_model_score.item() \n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna call with progress bar\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100) \n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (MAE): {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "file_path = r\"E:\\00_Thesis\\02_Nbeats\\SOH\\best\\02\\train500\\Epoch_Loss_Data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\", linestyle=\"-\", marker=\"o\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Validation Loss\", linestyle=\"--\", marker=\"s\")\n",
    "\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")  \n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
