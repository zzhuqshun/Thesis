{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import RMSE, MAE\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_parts(data: pd.DataFrame, parts: int = 15) -> Dict[str, pd.DataFrame]:\n",
    "    # Split data into parts\n",
    "    chunk_size = len(data) // parts\n",
    "    return {f\"{idx+1}\": data.iloc[idx * chunk_size:(idx + 1) * chunk_size] for idx in range(parts)}\n",
    "\n",
    "def load_data(data_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "    data_path = Path(data_dir)\n",
    "    all_data = {}\n",
    "\n",
    "    # Find all parquet files\n",
    "    parquet_files = list(data_path.glob(\"**/df*.parquet\"))\n",
    "    print(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "    for file_path in tqdm(parquet_files, desc=\"Processing cells\", unit=\"cell\"):\n",
    "        # Extract cell number from parent directory name\n",
    "        file_name = file_path.stem  \n",
    "        cell_number = file_name.replace('df_', '')  \n",
    "        cell_name = f'C{cell_number}'  \n",
    "        tqdm.write(f\"Processing {cell_name} ...\")\n",
    "            \n",
    "        # Load and process data\n",
    "        data = pd.read_parquet(file_path)\n",
    "        data['Absolute_Time[yyyy-mm-dd hh:mm:ss]'] = pd.to_datetime(data['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "        \n",
    "        # Select relevant columns\n",
    "        data = data[['Absolute_Time[yyyy-mm-dd hh:mm:ss]', 'Current[A]', 'Voltage[V]', \n",
    "                    'Temperature[°C]', 'SOH_ZHU','Q_sum', 'EFC']]\n",
    "        \n",
    "        # Resample to hourly\n",
    "        data.set_index('Absolute_Time[yyyy-mm-dd hh:mm:ss]', inplace=True)\n",
    "        data_hourly = data.resample('h').mean().reset_index()\n",
    "        \n",
    "        # Fill missing values\n",
    "        data_hourly.interpolate(method='linear', inplace=True)\n",
    "        data_hourly['SOH_ZHU'] = data_hourly['SOH_ZHU'].fillna(1)\n",
    "        \n",
    "        all_data[cell_name] = data_hourly\n",
    "\n",
    "    return all_data\n",
    "data_dir = \"../01_Datenaufbereitung/Output/Calculated/\"\n",
    "all_data = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(all_data):\n",
    "    # Combine all data for visualization\n",
    "    combined_data = pd.concat([data_dict for data_dict in all_data.values()])\n",
    "\n",
    "    # Visualization\n",
    "    fig, axs = plt.subplots(6, 1, figsize=(10, 18))\n",
    "\n",
    "    data_columns = [\n",
    "        ('Temperature[°C]', 'blue', 'Temperature Data Distribution', 'Temperature (°C)'),\n",
    "        ('Current[A]', 'orange', 'Current Data Distribution', 'Current (A)'),\n",
    "        ('Voltage[V]', 'green', 'Voltage Data Distribution', 'Voltage (V)'),\n",
    "        ('SOH_ZHU', 'purple', 'SOH Data Distribution', 'SOH'),\n",
    "        ('Q_sum', 'red', 'Q_sum Data Distribution', 'Q_sum'),\n",
    "        ('EFC', 'cyan', 'EFC Data Distribution', 'EFC')\n",
    "    ]\n",
    "\n",
    "    for i, (column, color, title, xlabel) in enumerate(data_columns):\n",
    "        axs[i].hist(combined_data[column], bins=50, color=color, alpha=0.7)\n",
    "        axs[i].set_title(title)\n",
    "        axs[i].set_xlabel(xlabel)\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "\n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# visualize_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data_ranges(data_dict: Dict[str, pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Inspect time ranges and value ranges for each battery in the data dictionary\n",
    "    \"\"\"\n",
    "    for cell_name, cell_data in data_dict.items():\n",
    "        print(f\"\\n=== {cell_name} ===\")\n",
    "        \n",
    "        # Get time range\n",
    "        time_range = (cell_data['Absolute_Time[yyyy-mm-dd hh:mm:ss]'].min(), cell_data['Absolute_Time[yyyy-mm-dd hh:mm:ss]'].max())\n",
    "        print(f\"Time Range: {time_range[0]} to {time_range[1]}\")\n",
    "        \n",
    "        # Get value ranges for each column\n",
    "        for column in ['SOH_ZHU', 'Current[A]', 'Voltage[V]', 'Temperature[°C]', 'Q_sum', 'EFC']:\n",
    "            values = cell_data[column]\n",
    "            print(f\"\\n{column}:\")\n",
    "            print(f\"Value Range: {values.min():.4f} to {values.max():.4f}\")\n",
    "            print(f\"Number of Data Points: {len(values)}\")\n",
    "\n",
    "# View all data ranges\n",
    "# print(\"All Data Ranges:\")\n",
    "# inspect_data_ranges(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cell_data(all_data: Dict[str, pd.DataFrame], train=13, val=1, test=1, parts=15) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"Splits the dataset into training, validation, and test sets, then further divides train and val into parts.\"\"\"\n",
    "    \n",
    "    cell_names = list(all_data.keys())\n",
    "    np.random.seed(773)\n",
    "    np.random.shuffle(cell_names)\n",
    "\n",
    "    train_cells = cell_names[:train]\n",
    "    val_cells = cell_names[train:train + val]\n",
    "    test_cells = cell_names[train + val:train + val + test]\n",
    "\n",
    "    print(f\"Cell split completed:\")\n",
    "    print(f\"Training set: {len(train_cells)} cells\")\n",
    "    print(f\"Validation set: {len(val_cells)} cells\")\n",
    "    print(f\"Test set: {len(test_cells)} cells\")\n",
    "\n",
    "    train_data = {}\n",
    "    for cell in train_cells:\n",
    "        split_data = split_data_into_parts(all_data[cell], parts=parts)\n",
    "        for part_idx, df_part in split_data.items():\n",
    "            part_name = f\"{cell}_{part_idx}\"\n",
    "            train_data[part_name] = df_part  \n",
    "\n",
    "    val_data = {cell: all_data[cell] for cell in val_cells}\n",
    "    test_data = {cell: all_data[cell] for cell in test_cells}\n",
    "\n",
    "    print(f\"Final dataset sizes:\")\n",
    "    print(f\"Training set: {len(train_data)} parts\")\n",
    "    print(f\"Validation set: {len(val_data)} full cells\")\n",
    "    print(f\"Test set: {len(test_data)} full cells\")\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_data, val_data, test_data = split_cell_data(all_data)\n",
    "\n",
    "# inspect_data_ranges(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_soh(data_dict: dict, title: str, figsize=(10, 7)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot each cell's SOH\n",
    "    for cell_name, cell_data in data_dict.items():\n",
    "        target = cell_data['SOH_ZHU']\n",
    "        plt.plot(cell_data['Absolute_Time[yyyy-mm-dd hh:mm:ss]'], target, label=cell_name)\n",
    "    \n",
    "    plt.title(f'{title} Set SOH Curves')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SOH')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all three datasets\n",
    "# plot_dataset_soh(train_data, \"Training\")\n",
    "# plot_dataset_soh(val_data, \"Validation\")\n",
    "# plot_dataset_soh(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_dicts(\n",
    "    train_dict: Dict[str, pd.DataFrame],\n",
    "    val_dict: Dict[str, pd.DataFrame],\n",
    "    test_dict: Dict[str, pd.DataFrame]\n",
    ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n",
    "    \n",
    "    # Concatenate all training data\n",
    "    train_concat = pd.concat(train_dict.values(), axis=0)\n",
    "    \n",
    "    # Initialize scalers\n",
    "    standard_scaler = StandardScaler()  \n",
    "    minmax_scaler   = MinMaxScaler(feature_range=(0, 1), clip=True) \n",
    "    \n",
    "    # Fit scalers\n",
    "    standard_scaler.fit(train_concat[['Current[A]']])\n",
    "    minmax_scaler.fit(train_concat[['Temperature[°C]', 'Voltage[V]', 'Q_sum', 'EFC']])\n",
    "    \n",
    "    def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        df_copy[['Current[A]']] = standard_scaler.transform(df_copy[['Current[A]']])\n",
    "        df_copy[['Temperature[°C]', 'Voltage[V]', 'Q_sum', 'EFC']] = minmax_scaler.transform(df_copy[['Temperature[°C]', 'Voltage[V]', 'Q_sum', 'EFC']])\n",
    "        return df_copy\n",
    "    \n",
    "    # Scale all datasets\n",
    "    train_scaled = {name: transform(df) for name, df in train_dict.items()}\n",
    "    val_scaled   = {name: transform(df) for name, df in val_dict.items()}\n",
    "    test_scaled  = {name: transform(df) for name, df in test_dict.items()}\n",
    "    \n",
    "    return train_scaled, val_scaled, test_scaled\n",
    "\n",
    "train_scaled, val_scaled, test_scaled = scale_data_dicts(train_data, val_data, test_data)\n",
    "# inspect_data_ranges(val_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(\n",
    "    data_dict: Dict[str, pd.DataFrame],\n",
    "    max_encoder_length: int = 24,   \n",
    "    max_prediction_length: int = 1  \n",
    ") -> TimeSeriesDataSet:\n",
    "    all_list = []\n",
    "    for part_name, df in data_dict.items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['Absolute_Time[yyyy-mm-dd hh:mm:ss]'] = pd.to_datetime(df_copy['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "        df_copy.sort_values('Absolute_Time[yyyy-mm-dd hh:mm:ss]', inplace=True)\n",
    "\n",
    "        df_copy['group_id'] = part_name\n",
    "        df_copy['time_idx'] = np.arange(len(df_copy))\n",
    "        all_list.append(df_copy)\n",
    "\n",
    "    big_df = pd.concat(all_list, ignore_index=True)\n",
    "    \n",
    "    dataset = TimeSeriesDataSet(\n",
    "        big_df,\n",
    "        time_idx=\"time_idx\",\n",
    "        group_ids=[\"group_id\"],\n",
    "        target=\"SOH_ZHU\",\n",
    "        time_varying_unknown_reals=[],  \n",
    "        # time_varying_known_reals=[\"EFC\"], \n",
    "        time_varying_known_reals=[\"Current[A]\", \"Voltage[V]\", \"Temperature[°C]\"],\n",
    "        # time_varying_known_reals=[\"Current[A]\", \"Voltage[V]\", \"Temperature[°C]\", \"Q_sum\", \"EFC\"],\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        scalers={}\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 8, 64, step=8)\n",
    "    lstm_layers = trial.suggest_int(\"lstm_layers\", 1, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.5, step=0.1)\n",
    "    attention_head_size = trial.suggest_int(\"attention_head_size\", 2, 8, step=2)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    max_encoder_length = trial.suggest_int(\"max_encoder_length\", 12, 48, step=12)\n",
    "    max_prediction_length = trial.suggest_int(\"max_prediction_length\", 1, 12, step=1)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    train_dataset = prepare_dataset(train_data, max_encoder_length=max_encoder_length, max_prediction_length=max_prediction_length)\n",
    "    val_dataset = prepare_dataset(val_data, max_encoder_length=max_encoder_length, max_prediction_length=max_prediction_length)\n",
    "\n",
    "    train_dataloader = train_dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=4, persistent_workers=True)\n",
    "    val_dataloader = val_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=4, persistent_workers=True)\n",
    "\n",
    "    # Create model with sampled hyperparameters\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_size=hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        dropout=dropout,\n",
    "        attention_head_size=attention_head_size,\n",
    "        loss=RMSE(),\n",
    "        logging_metrics=[MAE(), RMSE()],\n",
    "        optimizer=\"adam\",\n",
    "        \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=100,  # Reduce for faster tuning\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=0.1,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=20)],\n",
    "        logger=TensorBoardLogger(\"tft_optuna_logs\")\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def tune_hyperparameters(n_trials=100):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    return study.best_params\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_params = tune_hyperparameters(n_trials=100)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = best_params['max_encoder_length']\n",
    "max_prediction_length = best_params['max_prediction_length']\n",
    "train_dataset = prepare_dataset(train_data, max_encoder_length=max_encoder_length, max_prediction_length=max_prediction_length)\n",
    "val_dataset = prepare_dataset(val_data, max_encoder_length=max_encoder_length, max_prediction_length=max_prediction_length)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=4, persistent_workers=True)\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=4, persistent_workers=True)\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "tft_best = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset,\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    hidden_size=best_params[\"hidden_size\"],\n",
    "    lstm_layers=best_params[\"lstm_layers\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    attention_head_size=best_params[\"attention_head_size\"],\n",
    "    loss=RMSE(),\n",
    "    logging_metrics=[MAE(), RMSE()],\n",
    "    optimizer=\"adam\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=2,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10)],\n",
    "    logger=TensorBoardLogger(\"tft_final_logs\")\n",
    ")\n",
    "\n",
    "trainer.fit(tft_best, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "trainer.save_checkpoint(\"TFT_model_best.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = r'E:\\00_Thesis\\TFT\\results\\00basic\\TFT_model.ckpt'\n",
    "# tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "\n",
    "test_dataset = prepare_dataset(test_scaled, max_encoder_length=24, max_prediction_length=1)\n",
    "test_dataloader = test_dataset.to_dataloader(train=False, batch_size=32, num_workers=4, persistent_workers=True, drop_last=False)\n",
    "test_predictions = tft.predict(test_dataloader, mode=\"prediction\")\n",
    "\n",
    "\n",
    "y_pred = test_predictions.cpu().numpy().flatten()\n",
    "y_true = np.array(test_dataset.data[\"target\"]).flatten() \n",
    "print(f\"y_pred shape: {len(y_pred)}\")\n",
    "print(f\"y_true shape: {len(y_true)}\")\n",
    "\n",
    "min_len = min(len(y_pred), len(y_true))\n",
    "y_pred = y_pred[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = root_mean_squared_error(y_true, y_pred)\n",
    "print(f\" MAE = {mae:.3e}, RMSE = {rmse:.3e}\")\n",
    "\n",
    "# visualize the prediction\n",
    "time_index = np.array(test_dataset.data[\"time\"]).flatten()[:min_len]  \n",
    "\n",
    "pred_df = pd.DataFrame({\"time_idx\": time_index, \"true_SOH_ZHU\": y_true, \"predicted_SOH_ZHU\": y_pred})\n",
    "pred_df.sort_values(\"time_idx\", inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pred_df[\"time_idx\"], pred_df[\"true_SOH_ZHU\"], label=\"True SOH_ZHU\")\n",
    "plt.plot(pred_df[\"time_idx\"], pred_df[\"predicted_SOH_ZHU\"], label=\"Predicted SOH_ZHU\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"SOH_ZHU\")\n",
    "plt.title(\"TFT Model - SOH_ZHU Test Prediction\")\n",
    "plt.text(0.80, 0.85, f\"MAE = {mae:.3e}\\nRMSE = {rmse:.3e}\", \n",
    "         transform=plt.gca().transAxes, fontsize=12, \n",
    "         verticalalignment='top', bbox=dict(facecolor='white', alpha=0.7))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
